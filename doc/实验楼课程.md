# C++实现太阳系行星系统

OpenGL 包含了很多渲染函数，但是他们的设计目的是独立于任何窗口系统或操作系统的。因此，它自身并没有包含创建打开窗口或者从键盘或鼠标读取时间的函数，甚至连最基本的显示窗口的功能都没有，所以单纯只使用 OpenGL 是完全不可能创建一个完整的图形程序的。并且绝大多数程序都需要与用户进行交互（响应键盘鼠标等操作）。GLUT 则提供了这一便利。

GLUT 其实是 OpenGL Utility Toolkit 的缩写，它是一个处理 OpenGL 程序的工具库，主要负责处理与底层操作系统的调用及 I/O 操作。使用 GLUT 可以屏蔽掉底层操作系统 GUI 实现上的一些细节，仅使用 GLUT 的 API 即可跨平台的创建应用程序窗口、处理鼠标键盘事件等等。

安装GLUT

~~~
sudo apt-get update && sudo apt-get install freeglut3 freeglut3-dev
~~~







# C++网络编程

## 创建子进程

~~~
#include <iostream>
#include <unistd.h>

int main(int argc, char *argv[])
{
    pid_t pid;

    pid = fork();
    if(pid < 0)
    {
        std::cout << " fork() error." << std::endl;
        exit(1);
    }
    else if(pid == 0)
    {
        //进入子进程
        std::cout << "Init child process : pid = " << getpid() << std::endl;
        while(1)
        {
            //循环等待
            sleep(2);
        }
    }
    //父进程路径
    std::cout << "Parent process : pid = " << getpid() << std::endl;
    while(1)
    {
        //循环等待
        sleep(2);
    }

    return 0;
}
~~~

当进程完成工作后并没有正常结束而依旧占用系统资源的情况下就成为了僵尸进程。

其实产生僵尸进程的原因主要是子进程结束后需要父进程来进行收回，如果此时父进程不收回资源或者无法收回子进程资源就会导致子进程成为僵尸进程。



## 信号响应

在 Linux 系统中运行的程序是肯定会与信号发生关系的，比如在运行程序的过程中，使用键盘按下 `Ctrl+C` 或者 `Ctrl+\` 就会向当前终端运行的进程发生 `SIGINT` 和 `SIGQUIT` 信号。还有通常使用 `kill` 去杀死一个进程时，我们使用 `kill -9 <pid>` 的方式也是通过向 `<pid>` 进场发送 `SIGKILL` 信号来 `杀死` 进程的。



使用系统函数 `signal()` 就可以完成自定义信号处理函数的注册步骤，该函数描述如下：

~~~
#include <signal.h>

//注册新的信号处理函数，返回之前注册的函数指针
void (*signal(int signo, void(*func)(int)))(int);
~~~



# Python 实现图片转字符画

RGB 值映射到灰度值（注意这个公式并不是一个真实的算法，而是简化的 sRGB IEC61966-2.1 公式）

简化版公式

~~~gray ＝ 0.2126 * r + 0.7152 * g + 0.0722 * b
gray ＝ 0.2126 * r + 0.7152 * g + 0.0722 * b
~~~

安装 Python 图像处理库 pillow（PIL）：

~~~
$ sudo pip3 install --upgrade pip
$ sudo pip3 install pillow
~~~

argparse 库是用来管理命令行参数输入的。

~~~
from PIL import Image
import argparse
# 首先，构建命令行输入参数处理 ArgumentParser 实例
parser = argparse.ArgumentParser()

# 定义输入文件、输出文件、输出字符画的宽和高
parser.add_argument('file')     #输入文件
parser.add_argument('-o', '--output')   #输出文件
parser.add_argument('--width', type = int, default = 80) #输出字符画宽
parser.add_argument('--height', type = int, default = 80) #输出字符画高

# 解析并获取参数
args = parser.parse_args()

# 输入的图片文件路径
IMG = args.file

# 输出字符画的宽度
WIDTH = args.width

# 输出字符画的高度
HEIGHT = args.height

# 输出字符画的路径
OUTPUT = args.output

ascii_char = list("$@B%8&WM#*oahkbdpqwmZO0QLCJUYXzcvunxrjft/\|()1{}[]?-_+~<>i!lI;:,\"^`'. ")

def get_char(r,g,b,alpha = 256):

    # 判断 alpha 值
    if alpha == 0:
        return ' '

    # 获取字符集的长度，这里为 70
    length = len(ascii_char)
    
    # 将 RGB 值转为灰度值 gray，灰度值范围为 0-255
    gray = int(0.2126 * r + 0.7152 * g + 0.0722 * b)

    # 灰度值范围为 0-255，而字符集只有 70
    # 需要进行如下处理才能将灰度值映射到指定的字符上
    unit = (256.0 + 1)/length
    
    # 返回灰度值对应的字符
    return ascii_char[int(gray/unit)]

if __name__ == '__main__':
    
    # 打开并调整图片的宽和高
    im = Image.open(IMG)
    im = im.resize((WIDTH,HEIGHT), Image.NEAREST)# Nearest (最近相邻插值算法/最近邻法）重采样算法

    # 初始化输出的字符串
    txt = ""
    
    # 遍历图片中的每一行
    for i in range(HEIGHT):
        # 遍历该行中的每一列
        for j in range(WIDTH):
            # 将 (j,i) 坐标的 RGB 像素转为字符后添加到 txt 字符串
            txt += get_char(*im.getpixel((j,i)))
        # 遍历完一行后需要增加换行符
        txt += '\n'
    # 输出到屏幕
    print(txt)
    
    # 字符画输出到文件
    if OUTPUT:
        with open(OUTPUT,'w') as f:
            f.write(txt)
    else:
        with open("output.txt",'w') as f:
            f.write(txt)
~~~

 `im.getpixel((j,i))` 获取得到坐标 `(j,i)` 位置的 RGB 像素值（有的时候会包含 alpha 值），返回的结果是一个元组，例如 `(1,2,3)` 或者 `(1,2,3,0)`。我们使用 `*` 可以将元组作为参数传递给 get_char，同时元组中的每个元素都对应到 get_char 函数的每个参数。



# 深入浅出剖析Opencv视觉处理

linux环境安装相应库的命令

~~~c++
sudo -H python3 -m pip install opencv-python==4.2.0.34 numpy==1.18.5
~~~



## opencv基础使用

[cv2.VideoCapture读取视频或摄像头，并进行保存帧图像或视频_AI算法联盟-CSDN博客_cv2.videocapture](https://blog.csdn.net/weixin_40922285/article/details/102967331)

`cap = cv2.VideoCapture(0)`参数0表示默认为笔记本的内置第一个摄像头，如果需要读取已有的视频则参数改为视频所在路径路径。

~~~
cap = cv2.VideoCapture('video.mp4')
~~~

`cap.isOpened()`判断视频对象是否成功读取，成功读取视频对象返回True。

`ret,frame = cap.read()`按帧读取视频，返回值ret是布尔型，正确读取则返回True，读取失败或读取视频结尾则会返回False。

`cv2.waitkey`跟`cv2.imshow`后面表示图片显示的停止时间。

`cv2.waitKey(1000) & 0xFF == ord(‘q’) `

先解释下字面意思：

- `cv2.waitKey(1000)`：在1000ms内根据键盘输入返回一个值
- `0xFF` ：一个十六进制数
- `ord('q')` ：返回q的ascii码

### 读取视频并按帧进行保存代码

~~~
import numpy as np
import cv2
import os
 
def video2image(video_dir,save_dir):
    cap = cv2.VideoCapture(video_dir) #生成读取视频对象
    n = 1   #计数
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))    #获取视频的宽度
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))   #获取视频的高度
    fps = cap.get(cv2.CAP_PROP_FPS)    #获取视频的帧率
    fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))    #视频的编码
    # 定义视频输出
    #writer = cv2.VideoWriter("teswellvideo_02_result.mp4", fourcc, fps, (width, height))
    i = 0
    timeF = int(fps)     #视频帧计数间隔频率
    while cap.isOpened():
        ret,frame = cap.read() #按帧读取视频
        #到视频结尾时终止
        if ret is False :
            break
        #每隔timeF帧进行存储操作
        if (n % timeF == 0) :
            i += 1
            print('保存第 %s 张图像' % i)
            save_image_dir = os.path.join(save_dir,'%s.jpg' % i)
            print('save_image_dir: ', save_image_dir)
            cv2.imwrite(save_image_dir,frame) #保存视频帧图像
        n = n + 1
        cv2.waitKey(1) #延时1ms
    cap.release() #释放视频对象
 
#读取文件夹所有视频，每个视频按帧保存图像
def video2image_multi(video_path,save_path):
    video_list = os.listdir(video_path)
    
    for i in range(len(video_list)) :      
        video_dir = os.path.join(video_path,video_list[i])
        cap = cv2.VideoCapture(video_dir)
        fps = cap.get(cv2.CAP_PROP_FPS)     # 视频的帧率
        save_num = 0
        n = 1           #计数
        timeF = int(fps)          # 视频帧计数间隔频率
        while cap.isOpened():
            ret,frame = cap.read()
            if ret is False :
                break
            #每隔timeF帧进行存储操作
            if (n % timeF == 0) :
                save_num += 1
                save_image_dir = os.path.join(save_path,'%s_%s.jpg' % (i,save_num))
                cv2.imwrite(save_image_dir,frame)
            n = n + 1
            cv2.waitKey(1)
        cap.release()
        print('读取第 %s 个视频完成 ！！！' % i)
 
if __name__ == '__main__' :
 
    video_to_image(r'E:\AI\video.mp4', r'E:\AI\video2image')
~~~

通过视频的帧数间隔截取视频的每一帧，自己设置帧间隔为 20，即每隔 20 帧截取一帧图像，并将我们截取的每一帧保存到我们自定义的文件夹中。

```
import cv2
# 读取视频流
cap = cv2.VideoCapture("/home/shiyanlou/Code/video.mp4")
c = 1
# 指定写入帧率为20
frameRate = 20
while(True):
    ret, frame = cap.read()
    if ret:
        if(c % frameRate == 0):
            print("Start intercepting the video ：" + str(c) + " Frame")
            cv2.imwrite("/home/shiyanlou/Desktop/capture_image/" + str(c) + '.jpg', frame)
        c += 1
        cv2.waitKey(0)
    else:
        print("All frames have been saved")
        break
cap.release()
```

### 视频编解码

VideoWriter 用于视频的保存，具体函数表示如下：

~~~
<VideoWriter object> = cv.VideoWriter( filename, fourcc, fps, frameSize[, isColor] )
~~~

函数参数：

- filename：给要保存的视频起个名字；

- fourcc：指定视频编解码器的 4 字节代码；

  【（‘P’，‘I’，‘M’，‘1’）是 MPEG-1 编解码器】

  【（‘M’，‘J’，‘P’，'G '）是一个运动 jpeg 编解码器】

- fps：帧率；

- frameSize：帧大小。

[Video Codecs by FOURCC - fourcc.org](https://www.fourcc.org/codecs.php)

~~~

#创建编码器
fourcc = cv2.VideoWriter_fourcc('M', 'P', '4', 'V')#VideoWriter_fourcc为视频编解码器
# cv2.VideoWriter_fourcc('M', 'P', '4', 'V') MPEG-4编码 .mp4  要限制结果视频的大小，这是一个很好的选择。
# cv2.VideoWriter_fourcc('X','2','6','4')   MPEG-4编码  .mp4  想限制结果视频的大小，这可能是最好的选择。
# cv2.VideoWriter_fourcc('I', '4', '2', '0'),该参数是YUV编码类型，文件名后缀为.avi   广泛兼容，但会产生大文件
# cv2.VideoWriter_fourcc('P', 'I', 'M', 'I'),该参数是MPEG-1编码类型，文件名后缀为.avi
# cv2.VideoWriter_fourcc('X', 'V', 'I', 'D'),该参数是MPEG-4编码类型，文件名后缀为.avi  要限制结果视频的大小，这是一个很好的选择。
# cv2.VideoWriter_fourcc('T', 'H', 'E', 'O'),该参数是Ogg Vorbis,文件名后缀为.ogv
# cv2.VideoWriter_fourcc('F', 'L', 'V', '1'),该参数是Flash视频，文件名后缀为.flv

fps = 30.0
frameSize = (640,480)

#视频编码
out = cv2.VideoWriter('video.avi', codec, fps, frameSize)
out.write(frame)  #VideoCapture读入图片，写入videoWrite数据流

cap.release()
out.release()
~~~



## NumPy入门

![image-20211101145636686](E:\Document\Typora\img\image-20211101145636686.png)

创建数组

~~~
In [1]: import numpy as np                                                      

In [2]: np.array([1,1,2])                                                       
Out[2]: array([1, 1, 2])

In [3]: np.ones(6)                                                              
Out[3]: array([1., 1., 1., 1., 1., 1.])

In [4]: np.zeros(6)                                                             
Out[4]: array([0., 0., 0., 0., 0., 0.])

In [5]: np.random.random(4)                                                     
Out[5]: array([0.49026501, 0.43402364, 0.9814815 , 0.98152691])

~~~

数字图像处理中常用的数据结构有矩阵，链表，拓扑结构和关系结构。图像的数据结构用于目标表示和描述。

矩阵可以用来描述图像，可以表示黑白图像、灰度图像和彩色图像。矩阵中的一个元素表示图像的一个像素。矩阵描述黑白图像时，矩阵中的元素取值只有 0 和 1 两个值，因此黑白图像又叫二值图像或二进制图像。矩阵描述灰度图像时，矩阵中的元素由一个量化的灰度级描述，灰度级通常为 8 位，即 0-255 之间的整数，其中 0 表示黑色，255 表示白色。矩阵描述彩色图像时，需要利用三原色（RGB）进行表示。彩色图像由三原色红、绿、蓝组成，RGB 图像的每个像素都是由不同灰度级的红、绿、蓝描述的，每种单色的灰度描述同灰度图像的描述方式相同，且每个单色都对应着一个矩阵。

OpenCV 图像处理针对的对象就是矩阵，每一张图像对应着的就是一个矩阵，而 OpenCV 图像读取的数据格式就是 numpy 的 ndarray 数据格式。一个 ndarray 是（通常为固定大小）具有相同类型和大小的项目的多维容器，与 Python 中的其他容器对象一样，ndarray 可以通过对数组建立索引或切片（使用 N 个整数）的方法和属性来访问和修改内容。

下面以 BGR 格式为例介绍 Image 的数据结构。OpenCV 中图像读入的数据格式是 numpy 的 ndarray 数据格式，即 `BGR（蓝、绿、红）` 格式，取值范围是 [0,255]。如下图所示，分为三个维度：

![image-20211101145734230](E:\Document\Typora\img\image-20211101145734230.png)

第一维度：Height 高度，对应图片的 nRow 行数

第二维度：Width 宽度，对应图片的 nCol 列数

第三维度：Value 代表 BGR 三通道的值

OpenCV 能够处理图片的颜色通道顺序为为 BGR（blue，green，red），即每个像素点由 3 个值组成，这 3 个值分别代表 blue, green, red 的值。

介绍一种简单的生成 BGR 三原色图片的方法，具体实现代码如下：

~~~
import cv2
import numpy as np
def color():
    blue = np.zeros([300,300,3],dtype='uint8')
    blue[:,:,0] = 255
    green = np.zeros([300,300,3],dtype='uint8')
    green[:,:,1] = 255
    red = np.zeros([300,300,3],dtype='uint8')
    red[:,:,2] = 255
    cv2.imshow('blue',blue)
    cv2.imshow('green', green)
    cv2.imshow('red', red)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
if __name__=='__main__':
    color()
~~~

读取图片的BGR数据

![image-20211101154654769](E:\Document\Typora\img\image-20211101154654769.png)

### Array数组

Numpy（Numerical Python）提供了 python 对多维数组对象的支持：ndarray，具有矢量运算能力，快速、节省空间。numpy 支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。NumPy 数组的维数称为秩（rank），一维数组的秩为 1，二维数组的秩为 2，以此类推。在 NumPy 中，每一个线性的数组称为是一个轴（axes），秩其实是描述轴的数量。比如说，二维数组相当于是一个一维数组，而这个一维数组中每个元素又是一个一维数组。所以这个一维数组就是 NumPy 中的轴（axes），而轴的数量——秩，就是数组的维数。

~~~
In [5]: a = np.array([[1,2,3],[4,5,6]])                                         

In [6]: print(a)     		# 打印数组
[[1 2 3]
 [4 5 6]]

In [7]: print(a.ndim)       # 打印数组维度
2

In [8]: print(a.shape)		# 打印数组各个维度的长度
(2, 3)

In [9]: print(a.dtype)		# 打印数组元素类型
int64
~~~

**使用 zero/ones/empty 创建数组:根据 shape 来创建**

- ones 创建全 1 矩阵
- zeros 创建全 0 矩阵
- eye 创建单位矩阵
- empty 创建空矩阵（实际有值，未初始化）

~~~
In [13]: x = np.arange(7)		# 0-6                                            

In [14]: print(x)                                                               
[0 1 2 3 4 5 6]

In [15]: x=np.arange(0,6,2)		# 0-6，步长为2                                      

In [16]: print(x)                                                               
[0 2 4]

In [18]: x = np.arange(2,4)		# 2-4,步长为1                                                    

In [19]: print(x)                                                               
[2 3]
~~~



#### ndarry的矢量化计算

矢量化（ vectorization ）使得你不用编写循环就可以对数据进行批量运算。大小相等的数组之间的任何算术运算都会将运算应用到元素级。ndarray 矢量化运算包括数组与标量运算和数组与数组的运算。

- 矢量运算：相同大小的数组键间的运算应用在元素上
- 矢量和标量运算：“广播”— 将标量“广播”到各个元素

~~~
In [20]: arr = np.array([[1,2,3,4],[5,6,7,8]])                                  

In [21]: print(arr)                                                             
[[1 2 3 4]
 [5 6 7 8]]

In [22]: print(1/arr)                                                           
[[1.         0.5        0.33333333 0.25      ]
 [0.2        0.16666667 0.14285714 0.125     ]]

In [23]: print(0.5*arr)                                                         
[[0.5 1.  1.5 2. ]
 [2.5 3.  3.5 4. ]]

In [24]: print(arr.dtype)                                                       
int64

In [25]: print(arr+arr)                                                         
[[ 2  4  6  8]
 [10 12 14 16]]

In [26]: print(arr*arr)                                                         
[[ 1  4  9 16]
 [25 36 49 64]]

In [27]: print(arr-arr)                                                         
[[0 0 0 0]
 [0 0 0 0]]

In [28]: print(arr>arr)                                                         
[[False False False False]
 [False False False False]]
~~~

**索引**：获取数组中特定位置元素的过程

若元素个数为 n， 则索引下标可表示为 [0,1,2,...,n-1] 或 [-n,-(n-1),-(n-2),...,-2,-1]。

- 一维数组的索引：与 Python 的列表索引功能相似
- 多维数组的索引：每个维度一个索引值，逗号分隔

例如以下例子：

1. arr[r1:r2, c1:c2]
2. arr[1,1] 等价 arr【1】【1】
3. [:] 代表某个维度的数据

![image-20211101161837386](E:\Document\Typora\img\image-20211101161837386.png)

**切片**：获取数组元素子集的过程

切片可用三元素冒号分割：ar1[起始编号:终止编号（不含）:步长]，起始编号默认是 0，终止编号默认是 n，步长默认是 1。

- 一维数组的切片：与 Python 的列表切片功能相似

![image-20211101163914161](E:\Document\Typora\img\image-20211101163914161.png)

- 多维数组的切片：逗号分隔，每个维度同一维切片一样，用三冒号分隔，如果只有一个：表示选取整个维度





`dtype`类型  **astype 复制数组，并转换类型**

~~~
In [6]: x=np.array([1,1.6,2],dtype = np.float64)                                

In [7]: y=x.astype(np.int32)

In [8]: print(y)                                                                
[1 1 2]

In [9]: print(x)                                                                
[1.  1.6 2. ]

In [10]: z = y.astype(np.float64)                                               

In [11]: print(z)                                                               
[1. 1. 2.]

#字符串转为数值
In [12]: x = np.array(['1','2','3'],dtype=np.string_)                           

In [13]: y = x.astype(np.int32)                                                 

In [14]: print(x)                                                               
[b'1' b'2' b'3']

In [15]: print(y)                                                               
[1 2 3]
~~~

#### ndarray的布尔型索引

**布尔数组**：顾名思义，元素类型为布尔值的数组，同样可以是一维或者多维数组

![image-20211102100840859](E:\Document\Typora\img\image-20211102100840859.png)

**花式索引:使用整型数组作为索引**

**花式索引**是一个 Numpy 的术语，它指利用整数数组进行索引。

> 注意：使用花式索引的返回与上述两种索引不一样，花式索引返回的是副本。

一维数组索引

~~~
In [16]: x=np.array([1,2,3,4,5,6])                                              

In [17]: print(x[[0,1,2]])                                                      
[1 2 3]

In [18]: print(x[[-1,-2,-3]])                                                   
[6 5 4]
~~~

二维数组索引

~~~
In [19]: x=np.array([[1,2],[3,4],[5,6]])                                        

In [20]: print(x[[0,1]])                                                        
[[1 2]
 [3 4]]

In [21]: print(x[[0,1],[0,1]])    	## 打印x[0][0]和x[1][1]
[1 4]

In [22]: print(x[[0,1]][:,[0,1]])	# 打印01行的01列
[[1 2]
 [3 4]]

In [23]: print(x[np.ix_([0,1],[0,1])])	#同上 打印01行的01列
 [3 4]]

In [24]: x[[0,1],[0,1]]=[0,0]

In [25]: print(x)
[[0 2]
 [3 0]
 [5 6]]
~~~



#### 数组的合并

~~~
In [26]: x = np.array([[1, 2, 3], [4, 5, 6]]) 
    ...: y = np.array([[7, 8, 9], [10, 11, 12]]) 
    ...: print(np.concatenate([x, y], axis = 0))	#竖直组合
    ...: print(np.concatenate([x, y], axis = 1))	#水平组合
[[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]]
[[ 1  2  3  7  8  9]
 [ 4  5  6 10 11 12]]
~~~

~~~
In [27]: print(np.vstack((x, y))) # 垂直堆叠: 相对于垂直组合
    ...: print(np.hstack((x, y))) # 水平堆叠：相对于水平组合
[[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]]
[[ 1  2  3  7  8  9]
 [ 4  5  6 10 11 12]]

~~~

#### 数组的分割

- `np.split`(ndarray,数量，axis)----axis=0 表示行，axis=1 表示列
- `np.vsplit`(ndarray,数量) —竖直线切成几段
- `np.hsplit`(ndarray,数量)–水平线切成几段

![image-20211102103636186](E:\Document\Typora\img\image-20211102103636186.png)

**r_用于按行堆叠**

![image-20211102103654376](E:\Document\Typora\img\image-20211102103654376.png)

**c_用于按列堆叠**

![image-20211102103706214](E:\Document\Typora\img\image-20211102103706214.png)

#### random随机生成

~~~
np.random.random(4)
np.random.randn(3,2)
np.rand(25).reshape(5,5)
~~~

#### 引入 linalg 模块，线性代数

引入 linalg 模块并起名为 L

~~~
In [28]: import numpy.linalg as L                                               

In [29]: A = np.random.rand(25).reshape((5,5))                                  

In [30]: A.T		# 转置矩阵                     
Out[30]: 
array([[0.71135699, 0.12693614, 0.30613904, 0.53391079, 0.97108146],
       [0.25141325, 0.32547838, 0.36231434, 0.56820874, 0.08619641],
       [0.05204239, 0.26712257, 0.19794411, 0.64797665, 0.76074772],
       [0.71739389, 0.18993683, 0.43978126, 0.40511528, 0.05178839],
       [0.18396315, 0.87396037, 0.90080226, 0.92934912, 0.49434369]])

In [31]: L.inv(A) 	# 矩阵求逆 
Out[31]: 
array([[ 11.13091821,  30.47213368, -25.92055023,  -5.89851915,
          0.30739315],
       [ 17.84051889,  50.24568221, -45.29284508,  -5.69708246,
         -2.22569279],
       [-14.89397877, -41.90856268,  34.48775851,   8.47513691,
          0.85639663],
       [-14.68239336, -44.93313076,  38.63915776,   7.55200242,
          0.29555833],
       [ -0.51761367,   0.58043949,   1.69417132,  -1.25325056,
          0.45825338]])

In [32]: B=np.random.rand(15).reshape((5,3))                                    

In [33]: A.dot(B)	# 矩阵乘法
Out[33]: 
array([[0.89624029, 1.13876095, 0.86731951],
       [1.06397129, 1.28809099, 1.28727368],
       [1.20445561, 1.54282743, 1.46536641],
       [1.85146134, 2.00100897, 1.90373203],
       [1.52694508, 1.24840715, 1.25429205]])

In [34]: L.det(A)	# 行列式求解
Out[34]: -0.0030189432631719726
~~~



## HighGUI上位机开发

HightGui（high-level graphical user interface）是一个可以移植的图形工具包。OpenCV 将与操作系统，文件系统，摄像机之类的硬件进行交互的一些函数纳入 HighGui 库中，有了 HighGui，我们可以方便的打开窗口，显示图像，读出或者写入图像相关的文件（包含图像与视屏），处理简单的鼠标，光标和键盘事件。也可以使用 HighGui 创建其他一些很有用的控件，如滑动条，并把它加入窗口。

为了好理解，我们认为 OpenCV 中的 HighGui 可以分为三部分：

- 硬件相关部分
- 文件部分
- 图像用户界面部分

> 注：事实上，HighGui 的结构实现与我们所表述的不尽相同，HighGui 函数实际别划分为 “视频输入 \ 输出”，“图像输入 \ 输出”，“ GUI 工具” 三部分。

创建一个 HighGUI 窗口，使用到的函数为 namedWindow，将窗口命名为 image_HighGUI。创建窗口的代码具体如下：

```python
cv2.namedWindow('image_HighGUI')
```

当然函数 namedWindow 的参数不止一个，我们可以在其后面在加上一些参数（flags） 来实现窗口的一些设定。

> flags 其实是一个整数， 用这个二进制的特定的位， 来表示某个选项是 A 选项（二进制 0）还是 B 选项（二进制 1）。

- **设置选项 1： 窗口大小**

`WINDOW_NORMAL 1` 代表允许拖动窗口变换窗口大小。

`WINDOW_AUTOSIZE 0` 默认根据屏幕跟图片的大小，自动缩放。 不允许手动变化窗口大小。

- **设置选项 2： 设置宽高比**

`WINDOW_FREERATIO 256` 不固定宽高比。

`WINDOW_KEEPRATIO 0` 默认固定宽高比， 也就是窗口拖拽缩放， 必须保持原来的宽高比。

- **设置选项 3： 窗口 GUI 版本**

`WINDOW_GUI_NORMAL 16` 旧版窗口组件。 不支持 statusbar（状态栏） 跟 toolbar（工具栏）。

`WINDOW_GUI_EXPANDED 0` 默认新版本功能增强的 GUI 窗口。

我们可以通过**按位或**的方式，通过一个参数， 同时传入多个选项的值。

flags 的值默认为 0 ， 也就相当于 WINDOW_AUTOSIZE | WINDOW_KEEPRATIO | WINDOW_GUI_EXPANDED ，那么当我们创建窗口时可以写成如下的形式：

```python
cv2.namedWindow('image_HighGUI', flags=cv2.WINDOW_AUTOSIZE | cv2.WINDOW_KEEPRATIO | cv2.WINDOW_GUI_EXPANDED)
```

~~~
import numpy as np
import cv2
# 创建一个名字叫做 image_HighGUI 的窗口
cv2.namedWindow('image_HighGUI', cv2.WINDOW_NORMAL)
print("Please press any key to close the window")
key_pressed = cv2.waitKey(0)
cv2.destroyWindow('image_HighGUI')
~~~

### imread

当我们需要读入一张图片时，第一时间想到的是使用函数 `imread`，没错，但在 OpenCV 中 `imread` 函数的使用方式是 `cv2.imread()`，传入的第一个参数是图片的路径，第二个参数是图像颜色空间。

`cv2.imread()` 参数说明：

**第一个参数：图片路径+图片名**。OpenCV 的 `imread` 函数支持如下类型的图像载入。

具体包括如下类型的图片格式：

- Windows 位图：_.bmp, _.dib
- JPEG 文件：_.jpeg, _.jpg, *.jpe
- JPEG2000 文件：*.jp2
- PNG 图片：*.png
- 便携文件格式：_.pbm, _.pgm, *.ppm
- Sun raters 光栅文件：_.sr, _.ras
- TIFF 文件：_.tiff, _,tif

**第二个参数：载入标识符**。它指定了加载图像的颜色类型，默认值为 1。这个参数可以在 OpenCV 中表示图像格式的枚举体中取值。

- `CV_LOAD_IMAGE_UNCHANGED` —— 等价取值-1，这个标识在新版中已经废置，忽略。
- `CV_LOAD_IMAGE_GRAYSCALE` —— 等价于取值 0，始终将图像转换为灰度再返回。
- `CV_LOAD_IMAGE_COLOR` —— 等价取值 1，总是讲图像转换成彩色再返回。
- `CV_LOAD_IMAGE_ANYDEPTH` —— 等价取值 2，如果取这个标识，且载入图像深度为 16 位或者 32 位，则返回对应深度图像，否则转为 8 位图像再返回。
- `CV_LOAD_IMAGE_ANYCOLOR` —— 等价取值 4。

对于**参数 flags**，如果我们不在枚举体中取值，那么可以这样进行：

- flags>0 返回三个通道的彩色图像
- flags=0 返回灰度图像
- flags<0 返回包含 Alpha 通道的加载图像

`help(cv2.imread)`查看opencv导入格式

### imshow

在 OpenCV 的 HighGUI 展示图像， 需要使用到 `imshow` 函数，用法为 `cv2.imshow()`， 传入的第一个参数是窗口的名称，第二个参数是 Image 对象。

`cv2.imshow()` 参数说明：

- 第一个参数：窗口名称
- 第二个参数：需要显示的图像，Mat 类型。

如果窗口使用 `CV_WINDOW_AUTOSIZE` 标志创建，那么显示图像原始大小。否则按照图像的深度进行缩放，具体如下：

- 8-bit unsigned：原样输出
- 16-bit unsigned：使用像素值除以 256，也就是值得范围是[0,255x256]映射到[0,255]
- 32-bit unsigned：使用像素值除以 256，也就是值得范围是[0,255x256]映射到[0,255]
- 32-bit float point：像素要乘以 255，也就是范围[0,1]映射到[0,255]

### imwrite

`cv2.imwrite()` 参数说明：

- 第一个参数，需要写入的目标文件名，需要带上后缀。
- 第二个参数，InputArray 类型图像，一般填一个 Mat 类型图像数据。
- 第三个参数，表示为特定格式保存的参数编码，默认不用填。如果填的话，下面需要了解的地方。

**对于 JPEG 格式图片**，这个参数表示从 0 到 100 的图片质量（`CV_IMWRITE_JPEG_QUALITY`），默认值是 95.

JPEG 的第三个参数所代表的意思是图像质量 `cv2.IMWRITE_JPEG_QUALITY`， **取值范围**在 0-100， 默认是 95。

具体使用如下，`quality`为具体数值。

```python
cv2.imwrite('lena.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, quality])
```



**对于 PNG 格式图片**，表示压缩级别（`CV_IMWRITE_PNG_COMPRESSION`）从 0 到 9，较高的值意味着更小的尺寸和更长的压缩时间，默认为 3，`cmpi`为具体压缩级别。

~~~
cv2.imwrite('lena.png', img, [cv2.IMWRITE_PNG_COMPRESSION, cmpi])
~~~



**对于 PPM, PGM, 或者 PBM 格式的图片**，表示一个二进制格式标志（`CV_IMWRITE_PXM_BINARY`），取值为 0 或者 1，默认为 1.



### waitKey

在 OpenCV 的 HighGUI 里面的有 `waitKey` 键盘事件监听函数，下面我们就来讲讲 waitKey 函数的 API。

键盘事件监听函数 waitKey 的使用方法如下：

```python
cv2.waitKey(delay_ms)
```

这个函数主要有两个功能：

- 等待一个按键事件的发生
- 延时 delay_ms 个毫秒

waitKey 返回的数值是按下的按键字符，对应的 ASCII 编码。

这个函数大致的意思就是如果过了 n 个 ms 仍然没等到有按键事件发生， 就继续执行下面的函数， 类似于延时（delay）效果。

~~~
import cv2
# 创建一个名为image的窗口
cv2.namedWindow('image')
while True:
    # 等待按键事件发生
    key_code = cv2.waitKey(1000)
    if key_code != -1:
        print('key {} pressed- value={}'.format(chr(key_code), key_code))
        if chr(key_code) == 'q':
             # 按按键‘q’退出程序
            print('Quit')
            break
    else:
        print('no key pressed , wait 1s')
cv2.destroyWindow('image')
~~~

对于我们用过的 HighGUI 窗口，如果不需要对其进行销毁，在 OpenCV 中自带有 HighGUI 窗口的销毁，具体函数如下：

- 销毁所有窗口

在 opencv 中销毁所以窗口的代码如下：

```python
cv2.destroyAllWindows()
```

- 销毁单个窗口： 传入要销毁的窗口名称

在 opencv 中销毁单个窗口的代码如下：

```python
cv2.destroyWindow(window_name)
```

### 滑动条组件（ trackbar 回调函数的使用）

创建滑动条需要首先创建一个 trackbar，调用 createTrackbar 函数，具体形式如下：

```python
cv2.createTrackbar(trackbar_name,window_name,min_value,max_value,callback_func)
```

传入的主要参数：

- trackbar_name： 滑条的名称
- window_name： 滑条所在窗口 (window) 的名称
- min_value： 滑条最小值
- max_value： 滑条最大值
- callback_func： 回调函数，这个参数其实类似 C 语言中的函数指针，传入的是函数名称，每次滑条被拖动的时候，都会执行这个函数

#### 设置滑动条的位置

初始化滑动条的位置需要用到 setTrackbarPos 函数，具体形式如下：

```python
cv2.setTrackbarPos('trackbar_name','window_name', value)
```

传入的主要参数：

- trackbar_name： Trackbar 的名字
- window_name：Trackbar 所在的窗口的名字
- value：Trackbar 的初始值

#### 获取滑动条的位置

获取滑动条的位置除了可以在回调函数中获取 Trackbar 的取值， 还可以通过 getTrackbarPos 函数获取 Trackbar 的取值。

```python
value = cv2.getTrackbarPos('trackbar_name','window_name')
```

传入的主要参数：

- trackbar_name：Trackbar 的名字
- window_name：Trackbar 所在的窗口的名字

#### 单个滑动组件 Trackbar 实验

在环境终端的桌面上新建一个名为 `sigle_Trackbar.py` 的 python 文件，将以下代码拷贝到其中。

```python
import cv2
# 创建名为 image 的窗口
cv2.namedWindow('image')
value = None

# 回调函数 更新value的值
def update(x):
    global value
    value = x
    print('Update Value, value ={}'.format(value))

# 创建一个叫做 value_name 的滑动条对象
# 滑动条创建在 image 窗口之下
# 取值范围为 0-255, 回调函数为update
cv2.createTrackbar('value_name','image',0,255,update)
# 等待按键按下
cv2.waitKey(0)
# 销毁所有窗口
cv2.destroyAllWindows()
```

利用 `cd Desktop` 命令行进入得到环境桌面的路径下，输入 `ls` 命令行查看桌面的文件，可以看到有我们保存的代码文件 `sigle_Trackbar.py` 。再输入 `python3 sigle_Trackbar.py`，运行之后可看到如下效果：

![image-20211102141747384](E:\Document\Typora\img\image-20211102141747384.png)

#### 多个滑动组件 Trackbar 实验

在创建滑动组件时可以同时创建多个，对任意一个进行变更时都可以同步所有的值。在环境终端的桌面上新建一个名为 `multiple_Trackbar.py` 的 python 文件，将以下代码拷贝到其中。

```python
import cv2
# 创建名为 image 的窗口
cv2.namedWindow('image')
value = (0, 0, 0)

# 回调函数 更新value的值
def update(x):
    global value
    r_value = cv2.getTrackbarPos('R','image')
    g_value = cv2.getTrackbarPos('G', 'image')
    b_value = cv2.getTrackbarPos('B', 'image')
    value = (r_value, g_value, b_value)
    print('Update Value, value ={}'.format(value))

# 创建一个叫做 R / G / B 的滑动条对象
# 滑动条创建在 image 窗口之下
# 取值范围为 0-255, 回调函数为update
cv2.createTrackbar('R','image',0,255,update)
cv2.createTrackbar('G','image',0,255,update)
cv2.createTrackbar('B','image',0,255,update)

# 设置滑动条 R / G / B 的默认值为 125
cv2.setTrackbarPos('R','image',125)
cv2.setTrackbarPos('G','image',125)
cv2.setTrackbarPos('B','image',125)

# 等待按键按下
cv2.waitKey(0)
# 销毁所有窗口
cv2.destroyAllWindows()
```

利用 `cd Desktop` 命令行进入得到环境桌面的路径下，输入 `ls` 命令行查看桌面的文件，可以看到有我们保存的代码文件 `multiple_Trackbar.py` 。再输入 `python3 multiple_Trackbar.py`，运行之后可看到如下效果：

![image-20211102141804869](E:\Document\Typora\img\image-20211102141804869.png)



#### 终端输入灰度的值生成特定灰度的图片

可以通过 HighGUI 窗口去创建一个通过滑动组件改变特定灰度的灰度图，可以在终端输入自己设定的灰度值生成特定灰度的图片。

从创建一个特定灰度的灰度图， 到终端输入灰度的值生成特定灰度的图片。

创建特定灰度图像需要自己定义一个 `createGrayscaleCanvas` 函数用于创建灰度图的画布，为了满足需要，对读入的灰度值需要判断灰度值是否满足要求，如果满足要求，应用于该灰度值。

在环境终端的桌面上新建一个名为 `GrayCanvas.py` 的 python 文件，将以下代码拷贝到其中。

```python
import numpy as np
import cv2

# 初始化灰度图的画布
def createGrayscaleCanvas(width, height, color=255):
    canvas = np.ones((height, width), dtype="uint8")
    canvas[:] = color
    return canvas

# 判断灰度值是否合法
def is_gvalue_legal(gvalue):
    return not (gvalue < 0 or gvalue > 255)

#  读入灰度值
#  如果符合要求的话, 就生成对应的背景. 不合法就要求重新输入
def read_gvalue():
    read_done = False
    gvalue = None
    while not read_done:
        gvalue_str  = input("Please enter the grayscale value: ")
        gvalue = int(gvalue_str)
        read_done = is_gvalue_legal(gvalue)
        # 添加一个温馨小提示
        if not read_done:
            print("Tips: The value range is out of bounds. The value range of grayscale image is between 0 to 255")
    return gvalue

gvalue = read_gvalue()
canvas = createGrayscaleCanvas(500, 500, color=gvalue)
# 显示画布
cv2.imshow("canvas", canvas)
print("Press any key to end")
# 等待按键按下
cv2.waitKey(0)
# 销毁所有窗口
cv2.destroyAllWindows()
```

灰度值满足要求时，具体演示效果如下：

![img](https://doc.shiyanlou.com/courses/3020/1488760/4e3aa5828a4836dbeb87bfe2fe6e2d09-0)

灰度值不满足要求时，终端会报出错误：`Tips: The value range is out of bounds. The value range of grayscale image is between 0 to 255`，具体演示效果如下：

![img](https://doc.shiyanlou.com/courses/3020/1488760/ef17c6fc5b9b5cf2e2a4b5266f52eea6-0)

#### 滑动组件改变特定灰度的灰度图

首先创建一个名 `gray_value` 的 trackbar 组件。灰度值最小值是 0，最大值是 255。 当进行修改时，回调函数是 `updateImg`。具体创建形式如下：

```python
cv2.createTrackbar('gray_value','image',0,255,updateImg)
```

当然还需要更新画布，将我们改变的灰度值的图像上传到 image 窗口上。更新画布函数如下：

```python
def updateImg(gvalue):
    img = createGrayscaleCanvas(500, 500, color=gvalue)
    cv2.imshow('image',img)
```

在环境终端的桌面上新建一个名为 `Gray_trackbar.py` 的 python 文件，将以下代码拷贝到其中。

```python
import cv2
import numpy as np

# 初始化灰度图的画布
def createGrayscaleCanvas(width, height, color=255):
    canvas = np.ones((height, width), dtype="uint8")
    canvas[:] = color
    return canvas

# 更新画布
def updateImg(gvalue):
    img = createGrayscaleCanvas(500, 500, color=gvalue)
    # 显示更新后的图片
    cv2.imshow('image',img)
cv2.namedWindow('image')

# 初始化画布
updateImg(0)
cv2.createTrackbar('gray_value','image',0,255,updateImg)
print("Enter the experiment, e key to exit the program")
img = None

# 匹配按键事件，判断是否退出
while cv2.waitKey(0) != ord('e'):
    continue
# 销毁所有窗口
cv2.destroyAllWindows()
```

具体演示如下：

![img](https://doc.shiyanlou.com/courses/3020/1488760/ea617dd9849cfc24d6c41f63ac4b7c2f-0)

#### 滑动组件生成特定rgb图

基于前两个项目，做了个滑条生成rgb图

~~~
import cv2
import numpy as np

# 初始化灰度图的画布
def createRGBCanvas(width, height, r=255,g=255,b=255):
    canvas = np.zeros([height, width, 3], dtype="uint8")
    canvas[:,:,0] = b
    canvas[:,:,1] = g
    canvas[:,:,2] = r
    return canvas

# 回调函数 更新value的值
def update(x):
    global value
    r_value = cv2.getTrackbarPos('R', 'image')
    g_value = cv2.getTrackbarPos('G', 'image')
    b_value = cv2.getTrackbarPos('B', 'image')
    value = (r_value, g_value, b_value)
    print('Update Value, value ={}'.format(value))
    img = createRGBCanvas(500, 500, r_value, g_value, b_value)
    # 显示更新后的图片
    cv2.imshow('image',img)

cv2.namedWindow('image')
value = (0,0,0)
# 初始化画布
update(0)
cv2.createTrackbar('R','image',0,255,update)
cv2.createTrackbar('G','image',0,255,update)
cv2.createTrackbar('B','image',0,255,update)
print("Enter the experiment, e key to exit the program")
img = None

# 设置滑动条 R / G / B 的默认值为 125
cv2.setTrackbarPos('R','image',125)
cv2.setTrackbarPos('G','image',125)
cv2.setTrackbarPos('B','image',125)

# 匹配按键事件，判断是否退出
while cv2.waitKey(0) != ord('q'):
    continue
# 销毁所有窗口
cv2.destroyAllWindows()
~~~

#### 使用 HighGUI 实现按键功能

OpenCV 里面的按键组件其实不存在的，但是可以通过其他的方式去实现按键组件，以下有两种方法可以实现按键效果：

- 键盘事件监听(`waitKey`)；
- 改造滑动条组件(`Trackbar`)变成按键：Trackbar 有两个取值 0(逻辑假，按键未按下)和 1(逻辑真，按键按下)。

~~~
import cv2
import time
# 创建名为 image 的窗口
cv2.namedWindow('image')

# 提示按键已经按下
def do_something():
    print('Button Pressed!')
    print('Something done')

# 回调函数
def update(x):
    if x == 1:
        do_something()
        cv2.waitKey(500)
        cv2.setTrackbarPos('button', 'image', 0)
cv2.createTrackbar('button','image',0,1,update)
# 等待按键按下
cv2.waitKey(0)
# 销毁所有窗口
cv2.destroyAllWindows()
~~~

主要鼠标时间监听函数列表如下：

- `EVENT_MOUSEMOVE` 鼠标移动 Mouse Move
- `EVENT_LBUTTONDOWN` 鼠标左键点击 Left Button Down
- `EVENT_RBUTTONDOWN` 鼠标右键点击 Right Button Down
- `EVENT_MBUTTONDOWN` 鼠标中键点击 Middle Button Down
- `EVENT_LBUTTONUP` 鼠标左键抬起 Left Button Up
- `EVENT_RBUTTONUP` 鼠标右键抬起 Right Button Up
- `EVENT_MBUTTONUP` 鼠标中键抬起 Middle Button Up
- `EVENT_LBUTTONDBLCLK` 鼠标左键双击 Left Button Double Click
- `EVENT_RBUTTONDBLCLK` 鼠标右键双击 Right Button Double Click
- `EVENT_MBUTTONDBLCLK` 鼠标中键双击 Middle Button Double Click

鼠标事件的回调参数列表如下：

- `event` 鼠标事件类型， 如上文所述 EVENT_MOUSEMOVE 等。
- `x` 鼠标当前在窗口 Windows 下的 x 坐标
- `y` 鼠标在当前窗口 Windows 下的 y 坐标
- `flags` 鼠标事件发生过程值中的一些其他事件标志/状态
- `EVENT_FLAG_LBUTTON` 左键正在按下
- `EVENT_FLAG_RBUTTON` 右键正在被按下
- `EVENT_FLAG_MBUTTON` 中键正在被按下
- `EVENT_FLAG_CTRLKEY` CTRL 键正在被按下
- `EVENT_FLAG_SHIFTKEY` SHIFT 键正在被按下
- `EVENT_FLAG_ALTKEY` ALT 键正在被按下
- `param` 用户自定义的参数

例如，当我们要实现 CTRL + 鼠标左键， 移动鼠标绘制圆圈时，可以按照如下代码实现：

~~~
def onMouse(event,x,y,flags,param):
    print(flags)
    print(cv2.EVENT_FLAG_LBUTTON | cv2.EVENT_FLAG_CTRLKEY)
    # 判断事件是否为 Left Button Double Clicck
    if event == cv2.EVENT_MOUSEMOVE and flags == (cv2.EVENT_FLAG_LBUTTON | cv2.EVENT_FLAG_CTRLKEY ):
        cv2.circle(img,(x,y),20,(255,0,0),-1)
~~~

### 自制绘图板

~~~
import cv2
import numpy as np

isMouseLBDown = False # 判断鼠标是否摁下的标志
circleColor = (0, 0, 0) # 画笔的颜色
circleRadius = 5 # 画笔的粗细
lastPoint = (0, 0)

# 鼠标回调函数  绘制图像
# x, y 都是相对于窗口内的图像的位置
def draw_circle(event,x,y,flags,param):
    # 判断事件是否为 Left Button Double Clicck
    # print(event)
    global img
    global isMouseLBDown
    global color
    global lastPoint

    if event == cv2.EVENT_LBUTTONDOWN:
        # 检测到鼠标左键按下
        isMouseLBDown = True
        cv2.circle(img,(x,y), int(circleRadius/2), circleColor,-1)
        lastPoint = (x, y)
    elif event == cv2.EVENT_LBUTTONUP:
        # 检测到鼠标左键抬起
        isMouseLBDown = False
    elif event == cv2.EVENT_MOUSEMOVE:
        if isMouseLBDown:
            cv2.line(img, pt1=lastPoint, pt2=(x, y), color=circleColor, thickness=circleRadius) # cv2.line()用来画线
            lastPoint = (x, y)

# 更新颜色
def updateCircleColor(x):
    global circleColor
    global colorPreviewImg

    r = cv2.getTrackbarPos('Channel_Red','image')
    g = cv2.getTrackbarPos('Channel_Green','image')
    b = cv2.getTrackbarPos('Channel_Blue','image')

    circleColor = (b, g, r)
    colorPreviewImg[:] = circleColor

# 更新画笔的宽度
def updateCircleRadius(x):
    global circleRadius
    global radiusPreview

    circleRadius = cv2.getTrackbarPos('Circle_Radius', 'image')
    # 重置画布
    radiusPreview[:] = (255, 255, 255)
    # 绘制圆形
    cv2.circle(radiusPreview, center=(50, 50), radius=int(circleRadius / 2), color=(0, 0, 0), thickness=-1)

# 创建一个画布，并绑定窗口和鼠标回调函数
img = np.ones((512,512,3), np.uint8)
img[:] = (255, 255, 255)

# 用于预览画笔的颜色
colorPreviewImg = np.ones((100, 100, 3), np.uint8)
colorPreviewImg[:] = (0,  0, 0)

# 用于预览画笔的粗细
radiusPreview = np.ones((100, 100, 3), np.uint8)
radiusPreview[:] = (255, 255, 255)

# 用于展示绘图区域的窗口
cv2.namedWindow('image')
# 用于预览颜色的窗口
cv2.namedWindow('colorPreview')
# 用于预览画笔宽度的窗口
cv2.namedWindow('radiusPreview')

# 在window‘image’ 上创建一个滑动条，起名为Channel_XXX， 设定滑动范围为0-255,
# onChange事件回调 啥也不做
cv2.createTrackbar('Channel_Red','image',0,255,updateCircleColor)
cv2.createTrackbar('Channel_Green','image',0,255,updateCircleColor)
cv2.createTrackbar('Channel_Blue','image',0,255,updateCircleColor)

cv2.createTrackbar('Circle_Radius','image',1,20,updateCircleRadius)
# 设置鼠标事件回调
cv2.setMouseCallback('image',draw_circle)

while(True):
    cv2.imshow('colorPreview', colorPreviewImg)
    cv2.imshow('radiusPreview', radiusPreview)
    cv2.imshow('image',img)
    if cv2.waitKey(1) == ord('q'):
        break

cv2.destroyAllWindows()
cv2.imwrite("Paint.png",  img)
~~~



## 图像的色彩空间

颜色空间按照基本结构可以分两大类：

- 基色颜色空间
- 色、亮分离颜色空间

前者的典型是 RGB，还包括 CMY、CMYK、CIE XYZ 等；后者包括 YCC/YUV、Lab、以及一批“色相类颜色空间”。

颜色模型就是描述用一组数值来描述颜色的数学模型。例如最常见的 RGB 模型，就是用 RGB 三个数值来描述颜色。通常颜色模型分为两类：设备相关和设备无关。

- **设备无关的颜色模型**：这类颜色模型是基于人眼对色彩感知的度量建立的数学模型，例如上面提到的 CIE-RGB、CIE-XYZ 颜色模型，再比如由此衍生的 CIE-xyY、CIE-L*u*v、CIE-L*a*b 等颜色模型。这些颜色模型主要用于计算和测量。
- **设备相关的颜色模型**：以最长见的 RGB 模型为例，一组确定的 RGB 数值，在一个液晶屏上显示，最终会作用到三色 LED 的电压上。这样一组值在不同设备上解释时，得到的颜色可能并不相同。再比如 CMYK 模型需要依赖打印设备解释。常见的设备相关模型有：RGB、CMYK、YUV、HSL、HSB(HSV)、YCbCr 等。这类颜色模型主要用于设备显示、数据传输等。

在编写代码前还需要先配置好实验的环境，在终端下输入如下命令行代入实验需要用到的 python 库。

```python
sudo -H python3 -m pip install opencv-python==4.2.0.34 numpy==1.18.5 matplotlib
```

色彩空间变换在 OpenCV 中需要使用 `cvtColor` 函数，具体使用方法如下：

```python
Img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
```

其中，传入的 img 就是源图像，COLOR_BGR2RGB 是表示从哪个格式转变成另外哪种格式的图片。

在 OpenCV 中支持很多种颜色空间的转换，我们可以通过终端 `ipython` 进行查看：首先进入`ipython` 的终端， 然后引入 `cv2` 模块，敲入 `cv2.COLOR_` ，然后按 `Tab` 键，即可看到如图的所以的颜色转换的函数。

从 BGR 色彩空间的图片转换为 RGB 色彩空间的方法有很多种，可以通过 `cv2` 中的 `merge` 函数，也可以使用 `cvtColor` 函数中的 `cv2.COLOR_BGR2RGB` 参数，当然也可以使用 `numpy` 模块进行转换。

在环境终端的桌面上新建一个名为 `BGR2RGB.py` 的 python 文件，将以下代码拷贝到其中。

~~~
import cv2
import numpy as np
from matplotlib import pyplot as plt 
# Pyplot 是 Matplotlib 的子库，提供了和 MATLAB 类似的绘图 API。

img = cv2.imread('lena.jpg')
B, G, R = cv2.split(img)

#BGR转RGB，方法1(merge函数)
img_rgb1 = cv2.merge([R, G, B])

#BGR转RGB，方法2（COLOR_BGR2RGB）
img_rgb2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

#BGR转RGB，方法3（使用numpy进行转换）
img_rgb3 = img[:,:,::-1]


plt.figure('BGR_RGB')

#显示opencv读进来的img， 通道顺序BGR
plt.subplot(3,3,1), plt.imshow(img)
#显示B通道
plt.subplot(3,3,4), plt.imshow(B)
#显示B通道
plt.subplot(3,3,5), plt.imshow(G)
#显示B通道
plt.subplot(3,3,6), plt.imshow(R)
#显示将BGR转为RGB的图像，3种方法
plt.subplot(3,3,7), plt.imshow(img_rgb1)
plt.subplot(3,3,8), plt.imshow(img_rgb2)
plt.subplot(3,3,9), plt.imshow(img_rgb3)

plt.show()
~~~

`matplotlib.pyplot`模块提供了一个 subplot() 函数，它可以**均等地划分画布**，该函数的参数格式如下：

plt.subplot(nrows, ncols, index)

nrows 与 ncols 表示要划分几行几列的子区域（nrows*nclos表示子图数量），index 的初始值为1，用来选定具体的某个子区域。

例如： subplot(233)表示在当前画布的右上角创建一个两行三列的绘图区域（如下图所示），同时，选择在第 3 个位置绘制子图。

显示效果

![image-20211102161233806](E:\Document\Typora\img\image-20211102161233806.png)

#### HSV色彩空间

HSV 颜色模式是除了 RGB 颜色模式之外的另一种颜色模式，RGB 被广泛运用于计算机中，而 HSV 则用在**电视显示**方面。它更符合人们对**颜色的描述(什么颜色(H)\**，**深浅度如何(S)**，**亮度如何(V))**。

- `H`：**色相，色度，色彩**，也就是我们平时说的颜色。如红，黄，洋红等。在 HSV 模型中，用度数描述，其中红色对于 0 度，绿色对于 120 度，蓝色对应 240 度。
- `S`：**饱和度**，色彩的深浅度(0-100%)
- `V`：**色调，纯度**，色彩的亮度(0-100%)

##### RGB转HSV的算法

~~~
max=max(R,G,B) 
min=min(R,G,B) 
if R = max, H = (G-B)/(max-min) 
if G = max, H = 2 + (B-R)/(max-min) 
if B = max, H = 4 + (R-G)/(max-min) 
H = H * 60 
if H < 0, H = H + 360 
V=max(R,G,B) 
S=(max-min)/max
~~~

##### HSV转RGB的算法

~~~
if s = 0 
    R=G=B=V 
else 
    H /= 60; 
    i = INTEGER(H) 
    f = H - i 
    a = V * ( 1 - s ) 
    b = V * ( 1 - s * f ) 
    c = V * ( 1 - s * (1 - f ) ) 
    switch(i) 
        case 0: R = V; G = c; B = a; 
        case 1: R = b; G = v; B = a; 
        case 2: R = a; G = v; B = c; 
        case 3: R = a; G = b; B = v; 
        case 4: R = c; G = a; B = v;
        case 5: R = v; G = a; B = b;
~~~

##### scikit-image 和 pillow 

~~~
import numpy as np
from skimage import data,img_as_float
from PIL import Image
import matplotlib.pyplot as plt

def _prepare_colorarray(arr):
    arr = np.asanyarray(arr)

    if arr.ndim not in [3, 4] or arr.shape[-1] != 3:
        msg = ("the input array must be have a shape == (.., ..,[ ..,] 3)), " +
               "got (" + (", ".join(map(str, arr.shape))) + ")")
        raise ValueError(msg)

    return img_as_float(arr)

# RGB -- HSV 色相(Hue)、饱和度(Saturation)、明度(Value)
def rgb2hsv(rgb):
    arr = _prepare_colorarray(rgb)   #转化为float类型
    out = np.empty_like(arr) #生成与arr相同的0矩阵

    # -- V channel
    out_v = arr.max(-1) # 求每一行的最大值

    # -- S channel
    delta = arr.ptp(-1) # 求每一行的极差  (max - min)
    # Ignore warning for zero divided by zero
    old_settings = np.seterr(invalid='ignore') #对浮点类型的处理有异常进行忽略
    out_s = delta / out_v
    out_s[delta == 0.] = 0.

    # -- H channel
    # red is max
    idx = (arr[:, :, 0] == out_v)
    out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]

    # green is max
    idx = (arr[:, :, 1] == out_v)
    out[idx, 0] = 2. + (arr[idx, 2] - arr[idx, 0]) / delta[idx]

    # blue is max
    idx = (arr[:, :, 2] == out_v)
    out[idx, 0] = 4. + (arr[idx, 0] - arr[idx, 1]) / delta[idx]
    out_h = (out[:, :, 0] / 6.) % 1.
    out_h[delta == 0.] = 0.

    np.seterr(**old_settings)

    # -- output
    out[:, :, 0] = out_h
    out[:, :, 1] = out_s
    out[:, :, 2] = out_v

    # remove NaN
    out[np.isnan(out)] = 0
    return out

# HSV -- RGB
def hsv2rgb(hsv):
    arr = _prepare_colorarray(hsv)
    hi = np.floor(arr[:, :, 0] * 6)
    f = arr[:, :, 0] * 6 - hi
    p = arr[:, :, 2] * (1 - arr[:, :, 1])
    q = arr[:, :, 2] * (1 - f * arr[:, :, 1])
    t = arr[:, :, 2] * (1 - (1 - f) * arr[:, :, 1])
    v = arr[:, :, 2]

    hi = np.dstack([hi, hi, hi]).astype(np.uint8) % 6
    out = np.choose(hi, [np.dstack((v, t, p)),
                         np.dstack((q, v, p)),
                         np.dstack((p, v, t)),
                         np.dstack((p, q, v)),
                         np.dstack((t, p, v)),
                         np.dstack((v, p, q))])
    return out


if __name__ == '__main__':
    ima = Image.open('lena.jpg' )
    print(ima.size)
    img = np.asarray(ima)

    img_hsv = rgb2hsv(img)
    plt.subplot(1,2,1)
    plt.title('rgb——hsv')
    plt.imshow(img_hsv);

    img = hsv2rgb(img_hsv)
    plt.subplot(1,2,2)
    plt.title('hsv——rgb')
    plt.imshow(img);
    plt.show()
~~~

![image-20211102161802690](E:\Document\Typora\img\image-20211102161802690.png)

## 图像的仿射变换

本节主要针对图像的仿射变换（平移、旋转、缩放和翻转）的数学原理进行推导并且通过 python 实现对应的功能。

### 图像平移的数学推导及实现

简单来说，图像的本质可以看做一个三维矩阵，第一维为长度，第二维是宽度，第三维是通道数(RGB)，如果一张图在 python 中是一个变量 image，那么其长宽即 width， height = image.shape[:2]。

图像的平移就是在 xy 平面内对图像进行移动，所以该操作有两个自由度。其表达式为：

![image-20211102163012726](E:\Document\Typora\img\image-20211102163012726.png)

其中，(b_0*b*0,b_1*b*1)为偏移量，b_0*b*0 为 x 的偏移量，b_1*b*1 是 y 轴的偏移量，单位为像素。

例如，如果要将图像向右平移 10 个像素，向下平移 30 个像素，那么变换矩阵 M 表示为如下：

![image-20211102163817213](E:\Document\Typora\img\image-20211102163817213.png)

我们可以根据 minAreaRect 函数返回的数据结构，用来提取最小外接矩形区域， 以矩形中心 (cx, cy) 作为对原来图像旋转的中心点，旋转角度设定为 theta：

```python
# 声明旋转矩阵
rotateMatrix = cv2.getRotationMatrix2D((cx, cy), theta, 1.0)
# 获取旋转后的图像
rotatedImg = cv2.warpAffine(img, rotateMatrix, (img.shape[1], img.shape[0]))
```

![image-20211102172735747](E:\Document\Typora\img\image-20211102172735747.png)

~~~
'''
    利用minAreaRect绘制最小面积矩形并绘制
'''
import numpy as np
import cv2

# 读入黑背景下的彩色手写数字
img = cv2.imread("color_number_handwriting.png")
# 转换为gray灰度图
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# 寻找轮廓
contours, hier = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

for cidx,cnt in enumerate(contours):
    minAreaRect = cv2.minAreaRect(cnt)
    # 转换为整数点集坐标
    # rectCnt = np.int64(cv2.boxPoints(minAreaRect))
    ((cx, cy), (w, h), theta) = minAreaRect

    cx = int(cx)
    cy = int(cy)
    w = int(w)
    h = int(h)
    # 获取旋转矩阵
    rotateMatrix = cv2.getRotationMatrix2D((cx, cy), theta, 1.0)
    rotatedImg = cv2.warpAffine(img, rotateMatrix, (img.shape[1], img.shape[0]))
    pt1 = (int(cx - w/2), int(cy - h/2))
    pt2 = (int(cx + w/2), int(cy + h/2))
    # 原图绘制矩形区域
    cv2.rectangle(rotatedImg, pt1=pt1, pt2=pt2,color=(255, 255, 255), thickness=3)
    # 绘制中心点
    cv2.circle(rotatedImg, (cx, cy), 5, color=(255, 0, 0), thickness=-1)
    cv2.imwrite("minarearect_cidx_{}.png".format(cidx), rotatedImg)
~~~

运行结果

![image-20211102172743998](E:\Document\Typora\img\image-20211102172743998.png)

#### findContours

[opencv cv.findContours 函数详解 图像轮廓层级 图像轮廓检索方式详解 - 我坚信阳光灿烂 - 博客园 (cnblogs.com)](https://www.cnblogs.com/wojianxin/p/12602490.html)

contours, hierarchy = cv.findContours( image, mode, method[, contours[, hierarchy[, offset]]] )

1. 参数1：源图像
2. 参数2：轮廓的检索方式
3. 参数3：一般用 cv.CHAIN_APPROX_SIMPLE，就表示用尽可能少的像素点表示轮廓
4. contours：图像轮廓坐标，是一个链表

![image-20211102170841249](E:\Document\Typora\img\image-20211102170841249.png)

5. hierarchy：[Next, Previous, First Child, Parent]，文中有详细解释

+ **RETR_LIST**

这是最简单的一种寻找方式，它不建立轮廓间的子属关系，也就是所有轮廓都属于同一层级。

+ **RETR_TREE**

会完整建立轮廓的层级从属关系。

+ **RETR_EXTERNAL**

只寻找最高层级的轮廓

+ **RETR_CCOMP**

所有的轮廓只分为2个层级，不是外层的就是里层的



#### minAreaRect

1、minAreaRect函数

函数作用：

主要求得包含点集最小面积的矩形，，这个矩形是可以有偏转角度的，可以与图像的边界不平行

2、minAreaRect函数调用形式

C++: RotatedRect minAreaRect(InputArray points)
InputArray points：表示输入的点集

输出是矩形的四个点坐标


#### boundingRect

函数作用：

计算轮廓的垂直边界最小矩形，矩形是与图像上下边界平行的

首先介绍下cv2.boundingRect(img)这个函数

这个函数很简单，img是一个二值图，也就是它的参数；

返回四个值，分别是x，y，w，h；

x，y是矩阵左上点的坐标，w，h是矩阵的宽和高

然后利用`cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)画出矩行`

参数解释
第一个参数：img是原图
第二个参数：（x，y）是矩阵的左上点坐标
第三个参数：（x+w，y+h）是矩阵的右下点坐标
第四个参数：（0,255,0）是画线对应的rgb颜色
第五个参数：2是所画的线的宽度

```
rect = cv2.boundingRect(cnt)
(x, y, w, h) = rect
```



#### 提取手写数字图片样本

~~~~
import numpy as np
import cv2

# 读入黑背景下的彩色手写数字
img = cv2.imread("color_number_handwriting.png")
# 转换为gray灰度图
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# 寻找轮廓
contours, hier = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 声明画布 拷贝自img
canvas = np.copy(img)

for cidx,cnt in enumerate(contours):
    (x, y, w, h) = cv2.boundingRect(cnt)
    print('RECT: x={}, y={}, w={}, h={}'.format(x, y, w, h))
    # 原图绘制圆形
    cv2.rectangle(canvas, pt1=(x, y), pt2=(x+w, y+h),color=(255, 255, 255), thickness=3)
    # 截取ROI图像
    cv2.imwrite('/home/shiyanlou/image/' + "number_boudingrect_cidx_{}.png".format(cidx), img[y:y+h, x:x+w])

cv2.imwrite("number_boundingrect_canvas.png", canvas)
~~~~

运行结果

![image-20211102172758834](E:\Document\Typora\img\image-20211102172758834.png)

将上述图片统一变换为 15*25 大小的二值化图像。

~~~
import numpy as np
import cv2
from glob import glob

img_paths = glob('/home/shiyanlou/image/*.png')

# 新的维度为10×20
new_dimension = (15, 25)

for img_path in img_paths:
    # 读入灰度图
    img = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)
    img_name = img_path.split('/')[-1]
    # 缩放
    resized = cv2.resize(img, new_dimension)
    # 二值化图片
    ret,thresh = cv2.threshold(resized,10,255,0)

    cv2.imwrite('/home/shiyanlou/image/'+img_name,thresh)
~~~



### 图像仿射变换下的各种插值算法

常见插值方法有**拉格朗日插值法**、**分段插值法**、**样条插值法**。

- **拉格朗日插值多项式**：当节点数 n 较大时，拉格朗日插值多项式的次数较高，可能出现不一致的收敛情况，而且计算复杂。随着样点增加，高次插值会带来误差的震动现象称为龙格现象。
- **分段插值**：虽然收敛，但光滑性较差。
- **样条插值**:样条插值是使用一种名为样条的特殊分段多项式进行插值的形式。由于样条插值可以使用低阶多项式样条实现较小的插值误差，这样就避免了使用高阶多项式所出现的龙格现象，所以样条插值得到了流行。

> 该实验还需要用到 `scipy` 的资源包，因此还需要导入，在终端下输入命令：`sudo -H python3 -m pip install scipy`。

如下代码实现的是对应插值的曲线表示。在实验环境桌面新建一个名为 `interpolation_curve.py` 的文件，将下面的代码复制到这个 python 文件中。

~~~
import numpy as np
from scipy import interpolate
import pylab as pl

x=np.linspace(0,10,11)
#x=[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]
y=np.sin(x)
xnew=np.linspace(0,10,101)
pl.plot(x,y,"ro")

for kind in ["nearest","zero","slinear","quadratic","cubic"]:#插值方式
    # nearest：最邻近插值
    # zero：阶梯插值
    # slinear ：线性插值
    # "quadratic","cubic" 为2阶、3阶B样条曲线插值
    f=interpolate.interp1d(x,y,kind=kind)
    # ‘slinear’, ‘quadratic’ and ‘cubic’ refer to a spline interpolation of first, second or third order)
    ynew=f(xnew)
    pl.plot(xnew,ynew,label=str(kind))
pl.legend(loc="lower right")
pl.show()
~~~

**linespace**

+ linspace（x, y）产生一个有100个元素的行向量，其中的元素在区间[x, y]中等间隔分布。
+ linspace（x, y, n）产生x和y之间等间隔的n个数，如果n = 1，返回结果为y。

scipy.interpolate是插值模块，插值是离散函数逼近的重要方法，利用它可通过函数在有限个点处的取值状况，估算出函数在其他点处的近似值。与拟合不同的是，要求曲线通过所有的已知数据。计算插值有两种基本的方法：

对一个完整的数据集去拟合一个函数；
仿样内插法：对数据集的不同部分拟合出不同的函数，而函数之间的曲线平滑对接。
SciPy的interpolate模块提供了许多对数据进行插值运算的函数，范围涵盖简单的一维插值到复杂多维插值求解。

当样本数据变化归因于一个独立的变量时，就使用一维插值；反之样本数据归因于多个独立变量时，使用多维插值。

`interpolate.interp1d`返回一个函数，使用插值法查找新点的值。

显示结果

![image-20211103152140982](E:\Document\Typora\img\image-20211103152140982.png)

插值算法效果图

~~~
import cv2
import numpy as np
from matplotlib import pyplot as plt

img = np.uint8(np.random.randint(0,255,size=(5,5)))
height,width= img.shape

# 声明新的维度
new_dimension = (1000, 1000)

# 原始图像
plt.subplot(231)
plt.title("SRC Image")
plt.imshow(img,cmap='seismic')

# 最近邻插值 INTER_NEAREST
plt.subplot(232)
resized = cv2.resize(img, new_dimension, interpolation = cv2.INTER_NEAREST)
plt.title("INTER_NEAREST")
plt.imshow(resized,cmap='seismic')

# 线性插值 INTER_LINEAR
plt.subplot(233)
resized = cv2.resize(img, new_dimension, interpolation = cv2.INTER_LINEAR)
plt.title("INTER_LINEAR")
plt.imshow(resized,cmap='seismic')

# 区域插值 INTER_AREA
plt.subplot(234)
resized = cv2.resize(img, new_dimension, interpolation = cv2.INTER_AREA)
plt.title("INTER_AREA")
plt.imshow(resized,cmap='seismic')

# 三次样条插值 INTER_CUBIC
plt.subplot(235)
resized = cv2.resize(img, new_dimension, interpolation = cv2.INTER_CUBIC)
plt.title("INTER_CUBIC")
plt.imshow(resized,cmap='seismic')

# Lanczos插值 INTER_LANCZOS4
plt.subplot(236)
resized = cv2.resize(img, new_dimension, interpolation = cv2.INTER_LANCZOS4)
plt.title("INTER_LANCZOS4")
plt.imshow(resized,cmap='seismic')

plt.show()
~~~

显示结果

![image-20211103152316904](E:\Document\Typora\img\image-20211103152316904.png)

## 绘图

#### 创建白色画布（空白画布）

定义一个函数传入图像的宽度、高度和画布的颜色，空白画布颜色传入的 RGB 值为（255,255,255），即白色画布，具体函数如下：

```python
def Create_canvas(width, height, color=(255, 255, 255)):
    canvas = np.ones((height, width, 3), dtype="uint8")
    canvas[:] = color
    return canvas
```

在这个空白的画布上创建 500*500 颜色为纯白色的画布，可表示为：

```python
canvas = InitCanvas(500, 500, color=(0,0,0))
```

完整实现代码如下：

```python
import cv2
import numpy as np

def Create_canvas(width, height, color=(255, 255, 255)):
    canvas = np.ones((height, width, 3), dtype="uint8")
    canvas[:] = color
    return canvas

canvas = Create_canvas(500, 500, color=(255, 255, 255))
cv2.imshow('canvas', canvas)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

#### 创建彩色画布

##### 利用内置方法 merge 与 split 创建

我们初始化 BGR 的图片 canvas 之后需要将原来的图片进行通道分离，分别乘上 BGR 三个通道的整数值，然后将三个通道合并在一起，就得到我们想要的彩图纯色背景。那通道的分离需要用到的函数是 `cv2.split(img)`，具体用法如下：

```python
(channel_b, channel_g, channel_r) = cv2.split(canvas)
```

其中对应参数表示如下：

- `channel_b` 蓝色通道
- `channel_g` 绿色通道
- `channel_r` 红色通道

它们对应的都是二维的 ndarray 对象。例如，我们指定一种颜色，那么可以表示为 `color = (140, 30, 60))`。

> 注意, 我们这里的颜色指的 BGR 格式。

接下来分别将其乘上对应的值，如下：

```python
channel_b *= color[0]
channel_g *= color[1]
channel_r *= color[2]
```

然后我们将三个通道重新合并，需要用到的函数是 `cv2.merge`，具体用法如下：

```python
cv2.merge([channel_b, channel_g, channel_r])
```

> 注意：三个通道的矩阵以`list[]` 的方式传入 merge 函数.

综合以上初始化彩色背景的完整代码可表示为：

```python
import cv2
import numpy as np

# 初始化一个彩色的画布
def Create_canvas(width, height, color=(255, 255, 255)):
    canvas = np.ones((height, width, 3), dtype="uint8")
    # 将原来的三个通道抽离出来， 分别乘上各个通道的值
    (channel_b, channel_g, channel_r) = cv2.split(canvas)
    # 颜色的值与各通道的全1矩阵相乘
    channel_b *= color[0]
    channel_g *= color[1]
    channel_r *= color[2]
    # cv.merge 合并三个通道的值
    return cv2.merge([channel_b, channel_g, channel_r])

canvas = Create_canvas(500, 500, color=(140, 30,60))
cv2.imshow('canvas', canvas)

cv2.waitKey(0)
cv2.destroyAllWindows()
```

##### 利用 numpy 内置的索引创建

可以直接使用 numpy 的 ndarray 的索引的方法创建彩色画布，这种方法相对于上一种方法更加节约时间，且相对简单。

例如，`canvas[:,:,0]` 选中的是所有行和所有列像素元素的第一个值，也就是所有 B 通道的值。 然后对其进行赋值：

```python
canvas[:,:,0] = color[0]
```

利用 numpy 内置的索引创建彩色画布的完整使用的代码如下：

```python
import cv2
import numpy as np

def Create_canvas(width, height, color=(255, 255, 255)):
    canvas = np.ones((height, width, 3), dtype="uint8")
    # Blue
    canvas[:,:,0] = color[0]
    # Green
    canvas[:,:,1] = color[1]
    # Red
    canvas[:,:,2] = color[2]

    return canvas

canvas = Create_canvas(500, 500, color=(125, 50, 255))
cv2.imshow('canvas', canvas)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

### 绘制立体图形（条形图、立体柱状图、曲面图等）

在绘制立体图之前，利用 `matplotlib.figure.Figure` 创建一个图框：

```python
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
plt.show()
```

![image-20211103152912346](E:\Document\Typora\img\image-20211103152912346.png)

### 绘制立体条形图

绘制立体条形图需要用到的函数为 `bar()`，基本用法如下：

```python
ax.bar(left, height, zs=0, zdir='z', *args, **kwargs)
```

对应的参数表示如下：

- `x，y，zs = z`：数据
- `zdir`:条形图平面化的方向，具体可以对应代码理解。

绘制立体条形图的完整代码如下：

```python
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
for c, z in zip(['r', 'g', 'b', 'y'], [30, 20, 10, 0]):
    xs = np.arange(20)
    ys = np.random.rand(20)

    cs = [c] * len(xs)
    cs[0] = 'c'
    ax.bar(xs, ys, zs=z, zdir='y', color=cs, alpha=0.8)

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
```

![image-20211103152957903](E:\Document\Typora\img\image-20211103152957903.png)

~~~
add_subplot(self, *args, **kwargs)
~~~

add_subplot()方法，“向figure添加一个Axes作为一subplot布局的一部分。”类似于subplot，分别为row，col，index。

**zip()** 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。

### 绘制立体柱状图

绘制立体柱状图需要用到 bar3d 函数，具体用法如下：

```python
bar3d(x, y, z, dx, dy, dz, color='b', zsort='average', *args, **kwargs)
```

具体参数表示：

- x,y,z,dx,dy,dz 是列表，它们代表每个条的 x 和 y，z 位置；
- dx,dy,dz 代表条的深度,宽度和高度(x,y 和 z 的尺寸)。

绘制立体柱状图的完整代码如下：

~~~
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np

mpl.rcParams['font.size']=10
samples=25

 # 生成25个随机整数
x=np.random.normal(5,1,samples)
y=np.random.normal(3,0.5,samples)
fig=plt.figure(figsize=(12,8))
ax=fig.add_subplot(211,projection='3d')
hist,xedges,yedges=np.histogram2d(x,y,bins=10)
elements=(len(xedges)-1*len(yedges)-1)

# meshgrid把X和Y变成平方长度，比如原来都是4，经过meshgrid和ravel之后，长度都变成了16，因为网格点是16个
xpos,ypos=np.meshgrid(xedges[:-1]+0.25,yedges[:-1]+0.25)# 网格化坐标
xpos=xpos.flatten()
ypos=ypos.flatten()
zpos=np.zeros(100)
# 设置柱子属性
dx=0.1*np.ones_like(zpos)
dy=dx.copy()
dz=hist.flatten()
ax.bar3d(xpos,ypos,zpos,dx,dy,dz,color='b',alpha=0.4)
ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(0.5))
ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.4))
ax.zaxis.set_major_locator(mpl.ticker.MultipleLocator(0.6))
ax.set_xlabel('x axis')
ax.set_ylabel('y axis')
ax.set_zlabel('z axis')
plt.show()
~~~

![image-20211103154502294](E:\Document\Typora\img\image-20211103154502294.png)

### 绘制曲面图

绘制立体曲面图的具体代码如下：

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 准备数据
n = 1000
x, y = np.meshgrid(np.linspace(-3, 3, n),
                   np.linspace(-3, 3, n))

z = (1 - x / 2 + x ** 5 + y ** 3) * \
    np.exp(-x ** 2 - y ** 2)

# 绘制图片
fig = plt.figure("3D Surface", facecolor="lightgray")
plt.title("3D Surface", fontsize=18)

# 设置为3D图片类型
ax3d = Axes3D(fig)

ax3d.set_xlabel("X")
ax3d.set_ylabel("Y")
ax3d.set_zlabel("Z")
plt.tick_params(labelsize=10)

ax3d.plot_surface(x, y, z, cstride=20, rstride=20, cmap="jet")

plt.show()
```

![image-20211103154555338](E:\Document\Typora\img\image-20211103154555338.png)





# Pytorch深度学习入门

官方文档[torch — PyTorch 1.10.0 documentation](https://pytorch.org/docs/stable/torch.html)

## 张量

`torch.empty(width,height)`创建返回填充了未初始化数据的张量

`torch.rand(width,height)`创建随机初始化矩阵

`torch.zeros(width,height,dtype=torch.long)`创建0填充矩阵，数据类型为`long`

`x = torch.tensor([5.5，3.6])`使用现有数据初始化

`x = torch.new_ones(width,height,dtype=torch.double)`创建新张量

`x = torch.randn_like(x, dtype=torch.float)`覆盖dtype，数值为随机，类型发生变化

`x.size()`获取张量的size，返回类型为tuple

### 加法运算

~~~
torch.add(x,y)
torch.add(x,y,out=result)
y.add_(x) # 将x加到y
~~~

任何以下划线结尾的操作都会用结果替换原变量。例如：`x.copy_(y)`, `x.t_()`, 都会改变 `x`。

### 索引

和numpy一样`x[:,1]`

`torch.view`可以改变张量的维度和大小

~~~
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  # size -1 从其他维度推断

x.size(), y.size(), z.size()

(torch.Size([4, 4]), torch.Size([16]), torch.Size([2, 8]))
~~~

如果张量只有一个元素，使用 `.item()` 来得到 Python 数据类型的数值。

~~~
x = torch.randn(1)

x, x.item()

(tensor([0.8155]), 0.815525233745575)
~~~

### NumPy转换

~~~
a = torch.ones(5)
a
tensor([1., 1., 1., 1., 1.])
b = a.numpy()
b
array([1., 1., 1., 1., 1.], dtype=float32)
a.add_(1)
a, b
(tensor([2., 2., 2., 2., 2.]), array([2., 2., 2., 2., 2.], dtype=float32))
~~~

NumPy 数组转换成 PyTorch 张量时，可以使用 `from_numpy` 完成：

~~~
import numpy as np

a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
a, b
(array([2., 2., 2., 2., 2.]),
 tensor([2., 2., 2., 2., 2.], dtype=torch.float64))
~~~

## CUDA张量

CUDA 张量是能够在 GPU 设备中运算的张量。使用 `.to` 方法可以将 Tensor 移动到 GPU 设备中：

~~~
# is_available 函数判断是否有 GPU 可以使用
if torch.cuda.is_available():
    device = torch.device("cuda")          # torch.device 将张量移动到指定的设备中
    y = torch.ones_like(x, device=device)  # 直接从 GPU 创建张量
    x = x.to(device)                       # 或者直接使用 .to("cuda") 将张量移动到 cuda 中
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # .to 也会对变量的类型做更改
    
tensor([1.4566], device='cuda:0')
tensor([1.4566], dtype=torch.float64)
~~~

### Autograd 自动求导

PyTorch 中所有神经网络的核心是 [`autograd`](https://pytorch.org/docs/stable/autograd.html?highlight=autograd#module-torch.autograd)。我们先简单介绍一下这个包，然后训练一个神经网络。

[`autograd`](https://pytorch.org/docs/stable/autograd.html?highlight=autograd#module-torch.autograd)为张量上的所有操作提供了自动求导。它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行。`torch.Tensor` 是这个包的核心类。如果设置 `.requires_grad` 为 `True`，那么将会追踪所有对于该张量的操作。当完成计算后通过调用 `.backward()` 会自动计算所有的梯度，这个张量的所有梯度将会自动积累到 `.grad` 属性。这也就完成了自动求导的过程。

要阻止张量跟踪历史记录，可以调用 `.detach()` 方法将其与计算历史记录分离。为了防止跟踪历史记录（和使用内存），可以将代码块包装在 `with torch.no_grad():` 语句中。这一点在评估模型时特别有用，因为模型可能具有 `requires_grad=True` 的可训练参数，但是我们并不需要计算梯度。

自动求导中还有另外一个重要的类 `Function`。`Tensor` 和 `Function` 互相连接并生成一个非循环图，其存储了完整的计算历史。

如果需要计算导数，你可以在 `Tensor` 上调用 `.backward()`。 如果 `Tensor` 是一个标量（即它包含一个元素数据）则不需要为 `backward()` 指定任何参数。但是，如果它有多个元素，你需要指定一个 `gradient` 参数来匹配张量的形状。

接下来，我们创建一个张量并设置 `requires_grad=True` 用来追踪他的计算历史：

~~~
x = torch.ones(2, 2, requires_grad=True)
x
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
~~~

对张量进行操作，也就是计算过程：

~~~
y = x + 2
y
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
~~~

结果 `y` 已经被计算出来了，所以，`grad_fn` 已经被自动生成了。

~~~
y.grad_fn
<AddBackward0 at 0x7f151e546bd0>
~~~

然后，再对 `y` 进行操作：

~~~
z = y * y * 3
out = z.mean() # 求平均值

z, out

(tensor([[27., 27.],
         [27., 27.]], grad_fn=<MulBackward0>),
 tensor(27., grad_fn=<MeanBackward0>))
~~~

`.requires_grad_( ... )` 可以改变现有张量的 `requires_grad` 属性。 如果没有指定的话，默认输入的 flag 是 `False`。

~~~
a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)

False
True
<SumBackward0 object at 0x7f151e4d7090>
~~~

### 梯度

上面只是完成了梯度的自动追踪，下面通过反向传播打印对应节点的梯度。因为 `out` 是一个纯量 Scalar，`out.backward()` 等于 `out.backward(torch.tensor(1))`。

[`autograd`](https://pytorch.org/docs/stable/autograd.html?highlight=autograd#module-torch.autograd) 和 `Function` 的 [ *官方文档*](https://pytorch.org/docs/autograd)。



## 神经网络

### LeNet

PyTorch 中，我们可以使用 [`torch.nn`](https://pytorch.org/docs/stable/nn.html?highlight=torch nn#module-torch.nn)来构建神经网络。

上一讲已经讲过了 [`autograd`](https://pytorch.org/docs/stable/autograd.html?highlight=autograd#module-torch.autograd)，[`torch.nn`](https://pytorch.org/docs/stable/nn.html?highlight=torch nn#module-torch.nn) 依赖 [`autograd`](https://pytorch.org/docs/stable/autograd.html?highlight=autograd#module-torch.autograd) 来定义模型并求导。`nn.Module` 中包含了构建神经网络所需的各个层和 `forward(input)` 方法，该方法返回神经网络的输出。

下面给出一个示例网络结构，该网络也是经典的 [ *LeNet*](http://yann.lecun.com/exdb/lenet/)。

![image-20211103102013360](E:\Document\Typora\img\image-20211103102013360.png)

它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。

神经网络的典型训练过程如下：

1. 定义包含可学习参数（权重）的神经网络模型。
2. 在数据集上迭代。
3. 通过神经网络处理输入。
4. 计算损失（输出结果和正确值的差值大小）。
5. 将梯度反向传播回网络节点。
6. 更新网络的参数，一般可使用梯度下降等最优化方法。

~~~
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 3x3 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
net
~~~

测试随机输入32×32。注意，网络（LeNet）期望的输入大小是 32 \times 3232×32，如果使用 MNIST 数据集（28×28）来训练这个网络，需要把图片大小重新调整到32×32。

~~~
input = torch.randn(1, 1, 32, 32)
out = net(input)
out
~~~

模型中必须要定义 `forward` 函数，`backward` 函数（用来计算梯度）会被 [`autograd`](https://pytorch.org/docs/stable/autograd.html?highlight=autograd#module-torch.autograd) 自动创建。可以在 `forward` 函数中使用任何针对 Tensor 的操作。

`net.parameters()` 返回可被学习的参数（权重）列表和值：

~~~
params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1's .weight
~~~

将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：

~~~
net.zero_grad()
out.backward(torch.randn(1, 10))
~~~

注：

~~~
torch.nn 只支持小批量输入。整个 torch.nn 包都只支持小批量样本，而不支持单个样本。例如，nn.Conv2d 接受一个 4 维的张量，每一维分别是 sSamples x nChannels x Height x Width（样本数 x 通道数 x 高 x 宽）。如果是单个样本，需使用 input.unsqueeze(0) 来添加其它的维数解决问题。
~~~

### super

python中的`super(Net, self).__init__()`

首先找到Net的父类（比如是类NNet），然后把类Net的对象self转换为类NNet的对象，然后“被转换”的类NNet对象调用自己的init函数

对继承自父类的属性进行初始化。而且是用父类的初始化方法来初始化继承的属性。

### nn.Conv1d

一维卷积`nn.Conv1d`用于文本数据，只对宽度进行卷积，对高度不卷积。通常，输入大小为`word_embedding_dim * max_length`，其中，`word_embedding_dim`为词向量的维度，`max_length`为句子的最大长度。卷积核窗口在句子长度的方向上滑动，进行卷积操作。

~~~
class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
~~~

主要参数说明：

- `in_channels`：在文本应用中，即为词向量的维度
- `out_channels`：卷积产生的通道数，有多少个out_channels，就需要多少个一维卷积（也就是卷积核的数量）
- `kernel_size`：卷积核的尺寸；卷积核的第二个维度由`in_channels`决定，所以实际上卷积核的大小为`kernel_size * in_channels`
- `padding`：对输入的每一条边，补充0的层数

![image-20211103104359112](E:\Document\Typora\img\image-20211103104359112.png)

### **permute(dims)**

将tensor的维度换位。

参数： - __dims__ (int ..*) - 换位顺序

```python3
>>> x = torch.randn(2, 3, 5) 
>>> x.size() 
torch.Size([2, 3, 5]) 
>>> x.permute(2, 0, 1).size() 
torch.Size([5, 2, 3])
```



### 基础知识背景

[CS231n笔记：通俗理解CNN - mathor (wmathor.com)](https://wmathor.com/index.php/archives/1353/)

![image-20211103110146463](E:\Document\Typora\img\image-20211103110146463.png)

### nn.Conv2d二维卷积

~~~
class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
~~~

一般来说，二维卷积`nn.Conv2d`用于图像数据，对宽度和高度都进行卷积。

#### 定义

> class torch.**nn.Conv2d**(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)

#### 代码示例

假设现有大小为`32 x 32`的图片样本，输入样本的`channels`为1，该图片可能属于10个类中的某一类。CNN框架定义如下：

~~~
class CNN(nn.Module):
    def __init__(self):
        nn.Model.__init__(self)
 
        self.conv1 = nn.Conv2d(1, 6, 5)  # 输入通道数为1，输出通道数为6
        self.conv2 = nn.Conv2d(6, 16, 5)  # 输入通道数为6，输出通道数为16
        self.fc1 = nn.Linear(5 * 5 * 16, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 输入x -> conv1 -> relu -> 2x2窗口的最大池化
        x = self.conv1(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        # 输入x -> conv2 -> relu -> 2x2窗口的最大池化
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        # view函数将张量x变形成一维向量形式，总特征数不变，为全连接层做准备
        x = x.view(x.size()[0], -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
~~~

网络整体结构：[conv + relu + pooling] * 2 + FC * 3
 原始输入样本的大小：`32 x 32 x 1`

1. **第一次卷积**：使用6个大小为`5 x 5`的卷积核，故卷积核的规模为`(5 x 5) x 6`；卷积操作的`stride`参数默认值为`1 x 1`，32 - 5 + 1 = 28，并且使用ReLU对第一次卷积后的结果进行非线性处理，输出大小为`28 x 28 x 6`；
2. **第一次卷积后池化**：`kernel_size`为`2 x 2`，输出大小变为`14 x 14 x 6`；
3. **第二次卷积**：使用16个卷积核，故卷积核的规模为`(5 x 5 x 6) x 16`；使用ReLU对第二次卷积后的结果进行非线性处理，14 - 5 + 1 = 10，故输出大小为`10 x 10 x 16`；
4. **第二次卷积后池化**：`kernel_size`同样为`2 x 2`，输出大小变为`5 x 5 x 16`；
5. **第一次全连接**：将上一步得到的结果铺平成一维向量形式，5 x 5 x 16 = 400，即输入大小为`400 x 1`，W大小为`120 x 400`，输出大小为`120 x 1`；
6. **第二次全连接**，W大小为`84 x 120`，输入大小为`120 x 1`，输出大小为`84 x 1`；
7. **第三次全连接**：W大小为`10 x 84`，输入大小为`84 x 1`，输出大小为`10 x 1`，即分别预测为10类的概率值。

![image-20211103111528116](E:\Document\Typora\img\image-20211103111528116.png)

### torch.nn.MaxPool2d

卷积操作中 pool层是比较重要的，是提取重要信息的操作，可以去掉不重要的信息，减少计算开销。

~~~
class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
~~~

如果padding不是0，会在输入的每一边添加相应数目0  比如padding=1，则在每一边分别补0.

参数：

- kernel_size(int or tuple) - max pooling的窗口大小，
- stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size
- padding(int or tuple, optional) - 输入的每一条边补充0的层数
- dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数
- return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助
- ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作

### nn.Linear（）

PyTorch的`nn.Linear（）`是用于设置网络中的**全连接层的**，**需要注意在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为`[batch_size, size]`，不同于卷积层要求输入输出是四维张量**。

~~~
CLASS torch.nn.Linear(in_features,out_features,bias=True)
~~~

![image-20211103134958701](E:\Document\Typora\img\image-20211103134958701.png)

`in_features`指的是输入的二维张量的大小，即输入的[batch_size, size]中的size。
`out_features`指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。
从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。

### 总结

回顾一下到目前为止用到的类。

- `torch.Tensor`：自动调用 `backward()` 实现支持自动梯度计算的多维数组，并且保存关于这个向量的梯度。
- `nn.Module`：神经网络模块。封装参数、移动到 GPU 上运行、导出、加载等。
- `nn.Parameter`：变量，当把它赋值给一个 `Module` 时，被自动地注册为一个参数。
- `autograd.Function`：实现自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点。

至此，我们以及完成：

- 定义一个网络
- 处理输入，调用 `backword`。

接下来还需要：

- 计算损失。
- 更新网络权重。



### 损失函数

一个损失函数接受一对 `(output, target)` 作为输入，计算一个值来估计网络的输出和目标值相差多少。

[`torch.nn`](https://pytorch.org/docs/stable/nn.html?highlight=torch nn#module-torch.nn) 中有很多不同的 [ *损失函数*](https://pytorch.org/docs/nn.html#loss-functions)。[`nn.MSELoss`](https://pytorch.org/docs/stable/nn.html?highlight=nn mseloss#torch.nn.MSELoss) 是一个比较简单的损失函数，它可以用来计算输出和目标间的 [ *均方误差*](https://zh.wikipedia.org/zh-hans/均方误差)，例如：

~~~
output = net(input)
target = torch.randn(10)  # 随机值作为样例
target = target.view(1, -1)  # 使 target 和 output 的 shape 相同
criterion = nn.MSELoss()

loss = criterion(output, target)
loss
~~~

当我们添加 `loss` 计算之后，如果使用它 `.grad_fn` 属性，将得到如下所示的计算图：

~~~
input → conv2d → relu → maxpool2d → conv2d → relu → maxpool2d
      → view → linear → relu → linear → relu → linear
      → MSELoss
      → loss
~~~

所以，当我们调用 `loss.backward()` 时，会针对整个图执行微分操作。图中所有具有 `requires_grad=True` 的张量的 `.grad` 梯度会被累积起来。为了说明该情况，我们回溯几个步骤：

~~~
print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU
~~~

### 反向传播

调用 `loss.backward()` 获得反向传播的误差。但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。现在，我们将调用 `loss.backward()`，并查看 `conv1` 层的偏差（bias）项在反向传播前后的梯度。下方的代码只能执行一次。

~~~
net.zero_grad()  # 清除梯度

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
~~~

### 更新权重

至此，剩下的最后一件事，那就是更新网络的权重。

在实践中最简单的权重更新规则是随机梯度下降（SGD）：

![image-20211103144735764](E:\Document\Typora\img\image-20211103144735764.png)

我们可以使用简单的 Python 代码实现这个规则：

~~~
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
~~~

当你想使用其他不同的优化方法，如 SGD、Nesterov-SGD、Adam、RMSPROP 等来更新神经网络参数时。可以借助于 PyTorch 中的 [`torch.optim`](https://pytorch.org/docs/stable/optim.html?highlight=torch optim#module-torch.optim) 快速实现。使用它们非常简单：

~~~
import torch.optim as optim

# 创建优化器
optimizer = optim.SGD(net.parameters(), lr=0.01)

# 执行一次训练迭代过程
optimizer.zero_grad()  # 梯度置零
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()  # 更新
loss
~~~

## 训练一个分类器

上面，你已经看到如何去定义一个神经网络，计算损失值和更新网络的权重。接下来，我们实现一个图像分类神经网络。

一般情况下处理图像、文本、音频和视频数据时，可以使用标准的 Python 来加载数据为 NumPy 数组。然后把这个数组转换成`torch.*Tensor`。

- 图像可以使用 Pillow, OpenCV。
- 音频可以使用 SciPy, librosa。
- 文本可以使用原始 Python 和 Cython 来加载，或者使用 NLTK 或 SpaCy 处理。

特别地，对于图像任务，PyTorch 提供了专门的包 [`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html?highlight=torchvision#)，它包含了处理一些基本图像数据集的方法。这些数据集包括 Imagenet, CIFAR10, MNIST 等。除了数据加载以外，[`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html?highlight=torchvision#) 还包含了图像转换器，`torchvision.datasets` 和 `torch.utils.data.DataLoader` 数据加载器。

[`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html?highlight=torchvision#)不仅提供了巨大的便利，也避免了代码的重复。接下来，我们使用 CIFAR10 数据集完成分类器训练。该数据集有如下 10 个类别：airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck。CIFAR-10 的图像都是 3×32×32 ，即 3 个颜色通道，32 \times 3232×32 像素。

![image-20211103145038528](E:\Document\Typora\img\image-20211103145038528.png)

训练一个图像分类器，基本流程如下：

1. 使用 [`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html?highlight=torchvision#) 加载和归一化 CIFAR10 训练集和测试集。
2. 定义一个卷积神经网络。
3. 定义损失函数。
4. 在训练集上训练网络。
5. 在测试集上测试网络。

#### 读取和归一化CIFAR10

使用 [`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html?highlight=torchvision#) 可以非常容易地加载 CIFAR10。[`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html?highlight=torchvision#) 的输出是 `[0,1]` 的 PILImage 图像，我们把它转换为归一化范围为 `[-1, 1]` 的张量。

~~~
import torchvision
import torchvision.transforms as transforms

# 图像预处理步骤
transform = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
# 训练数据加载器
trainset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(
    trainset, batch_size=4, shuffle=True, num_workers=2)
# 测试数据加载器
testset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(
    testset, batch_size=4, shuffle=False, num_workers=2)
# 图像类别
classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

trainloader, testloader
~~~

可视化其中的一些训练图像。

%matplotlib具体作用是当你调用matplotlib.pyplot的绘图函数plot()进行绘图的时候，或者生成一个figure画布的时候，可以直接在你的python console里面生成图像

~~~
import matplotlib.pyplot as plt
%matplotlib inline


def imshow(img):
    # 展示图像的函数
    img = img / 2 + 0.5  # 反向归一化
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))


# 获取随机数据
dataiter = iter(trainloader)
images, labels = dataiter.next()

# 展示图像
imshow(torchvision.utils.make_grid(images))
# 显示图像标签
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))
~~~

#### 定义一个卷积神经网络

从之前的神经网络一节复制神经网络代码，并修改输入为 3 通道图像。

~~~
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
net
~~~

#### 定义损失函数和优化器

我们使用交叉熵作为损失函数，使用带动量的随机梯度下降完成参数优化。

~~~
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
optimizer
~~~

#### 训练网路

有趣的训练过程开始了。只需在数据迭代器上循环，将数据输入给网络，并优化。由于使用了卷积神经网络，该训练时间较长，请耐心等待。

~~~
for epoch in range(1):  # 迭代一次
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # 获取输入
        inputs, labels = data
        # 梯度置 0
        optimizer.zero_grad()
        # 正向传播，反向传播，优化
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        # 打印状态信息
        running_loss += loss.item()
        if i % 200 == 199:    # 每 200 批次打印一次
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 200))
            running_loss = 0.0
print('Finished Training.')
~~~

#### 在测试集上测试网络

我们在整个训练集上进行了训练，但是需要检查网络是否从数据集中学习到有用的东西。一般情况下，可以通过预测神经网络输出的类别标签与实际情况标签进行对比来进行检测。如果预测正确，我们把该样本添加到正确预测列表。

第一步，显示测试集中的图片并熟悉图片内容。

```
dataiter = iter(testloader)
images, labels = dataiter.next()

# 显示图片
imshow(torchvision.utils.make_grid(images))
print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))
```

让我们看看神经网络认为以上图片是什么。

```python
outputs = net(images)
outputs

```

输出是 10 个标签的权重。一个类别的权重越大，神经网络越认为它是这个类别。所以让我们得到最高权重的标签。

```python
_, predicted = torch.max(outputs, 1)

print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))

```

结果看来不错。接下来让看看网络在整个测试集上的结果如何。

```python
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d%%' %
      (100 * correct / total))

```

结果看起来不错，至少比随机选择要好，随机选择的正确率为 10%。似乎网络学习到了一些东西。

那么，在识别哪一个类的时候好，哪一个不好呢？

```python
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()
        for i in range(4):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1

for i in range(10):
    print('Accuracy of %5s : %2d%%' %
          (classes[i], 100 * class_correct[i] / class_total[i]))

```

如上所示，得到了在单个类别上的预测准确度。
