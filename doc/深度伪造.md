# AI视频换脸之deepfake技术综述



## 1. 重要节点

2014.06：提出生成对抗网络（GAN），在图片创建方面取得重大突破，之前的AI算法可以较好的分类图片但创建图片困难；

2017.07：提出一种使用RNN 的LSTM（Long Short-Term Memory）学习口腔形状和声音之间关联性的方法，仅通过音频即可合成对应的口部特征；

2017.10：提出一种基于 GAN 的自动化实时换脸技术；

2017.12：出现deepfake色情视频，名人的面孔被换成色情演员的面孔，由Reddit用户使用自动编码器-解码器配对结构开发（FakeApp前身）；

2018.04：出现美国前总统奥巴马的deepfake演讲，将声优的口型（含模仿的配音）替换到奥巴马演讲视频中，使用了FakeApp；

2018.08：提出一种将源视频中的运动转移到另一个视频中目标人的方法，而不仅是换脸（效果imperfect）；

2019.03：提出一种控制图片生成器并能编辑造假图片各方面特性的方法，比如肤色、头发颜色和背景内容，不同于之前的假人图片生成方法，这是一种重大突破；

2019.05：提出一种真实头部说话神经模型的少样本对抗学习，基于GAN元学习，该模型基于少量图像（few-shot）训练后，向其输入一张人物头像，可以生成人物头像开口说话的动图；

2019.06：出现“一键式”智能脱衣软件 Deepnude，迫于舆论压力，开发者快速下架。

2020：主要进展在提高原生分辨率、提升deepfake制作效果方面。



## 2. 技术分类

deepfake一般可划分四类：重现（reenactment）、替换（replacement）、编辑（editing）、合成（synthesis）。

尽管人脸编辑和合成研究火热，但重现和替换才是最大的隐患，它们可以让攻击者控制身份和欺骗。

### 2.1 重现（reenactment）

重现使用源身份Xs驱动目标身份Xt，使得Xt的**行为**和Xs一样，包括表情、嘴部、眼部、头部及躯干。

l 表情重现

让Xt模仿Xs的表情，可用于电影行业中后期调整演员的表情表演、教育行业中对历史人物表情再现讲解事物。

l 嘴部重现

也称“配音”，Xt的嘴部由Xs驱动，或者是包含语音的音频输入，可用于将逼真的语音配音成另一种语言并进行编辑。

l 眼部重现

Xs视线驱动Xt眼睛的方向和眼睑的位置，可用于改善照片质量或在视频采访中自动保持眼神交流。

l 头部重现

Xt头部位置由Xs驱动，可用于在安全录像中对人脸进行朝向转正，并用作改善人脸识别软件的手段。

l 身体重现

也称姿态迁移和人体姿势合成，与面部重现类似，是身体躯干的重现。

![image-20210827094806673](E:\Document\Typora\img\image-20210827094806673.png)

​                                              人脸重现、替换、编辑、合成分别生成的deepfake示例

攻击模型：攻击者能用其假冒身份，假冒他人说或做，比如诽谤、散布错误信息、篡改证据、骗取信任诈骗、生成虚假材料勒索等。



### 2.2 替换（replacement）

替换使用源身份Xs的内容替换目标身份 Xt，使得目标**身份**变成了源身份Xs。

l 转移（transfer）

使用Xs的内容替换 Xt，常见是面部转移（即用Xs的脸直接覆盖 Xt的面部），在时尚行业中用于不同服装中的个人虚拟试穿。

l 交换（swap）

由Xt驱动Xs的内容转移到 Xt（从效果上看，Xt原有的面部表情等会被保留），最流行的是“换脸”，比如将某人的身份同名人的身份交换来产生讽刺或不良的内容效果、一种健康用途是在公共场合或平台匿名化身份以代替模糊或像素化。

攻击模型：替换技术因其有害应用而闻名，比如攻击者将受害者的脸换到色情女演员的身体上，以侮辱、诽谤和勒索受害者，还可将一个人的脸转移到一个看起来相似的身体上充分再现此人。

### 2.3 编辑（editing）

编辑是指添加、更改或删除目标身份的**属性**，比如，更换目标对象的发型、衣服、胡须、年龄、体重、颜值、眼镜和种族等属性。

攻击模型：攻击者可以使用相同的过程来建立虚假的角色误导他人，比如，可以使一个患病的领导者看起来健康的，震慑“敌方”，一个不道德的用途是去除受害者的着装使其裸露，即所谓的一键脱衣。



### 2.4 合成（synthesis）

合成是指在**没有目标身份作为基础**的情况下创建deepfake角色，类似直接用GAN或者其它生成模型生成人脸，没有明确的target。人脸和身体合成技术可以创建影视素材，生成电影和游戏角色。

攻击模型：攻击者可以在线创建虚假角色进行诈骗等违法活动。



## 3. 视觉深度伪造生成技术

### 3.1、深度伪造生成网络

通常使用5种神经网络的变种或组合构建深度伪造生成网络：编码－解码网络（Encoder-Decoder, ED）、卷积神经网络（Convolutional neural network, CNN）、生成对抗网络（Generative adversarial networks, GAN）、图像转换网络（Pix2Pix, CycleGAN）、递归神经网络（Recurrent neural network, RNN）。

#### 3.1.1 编码－解码网络（ED）

![image-20210827101112766](E:\Document\Typora\img\image-20210827101112766.png)

网络至少包含一个编码器En和一个解码器De，连接编码和解码的中间层较窄，因此当训练De(En(x)) = xg时，网络将被迫学习、汇总训练样本的高层语义概念。

给定x的分布X，En(x) = e，通常称e为编码或嵌入(embedding)，而E = En(X) 被称为“潜在空间(latent space)”。

Deepfake技术通常使用多个编码器或解码器，并操纵编码来影响输出 xg。如果编码器和解码器是对称的，并且以目标 De(En(x)) = x训练网络，则该网络称为自动编码器（Autoencoders），输出是x的重建。

ED的另一种特殊类型是变分自动编码器（VAE），其中编码器学习给定X的解码器后验分布。VAE比自动编码器在生成内容方面更好，因为潜在空间中的表征可以被更好地解耦。

##### 3.1.1.1 自动编码器（Autoencoders）

自动编码器提取面部图像的潜在特征，使用解码器重建面部图像，为了在源图像和目标图像之间交换面部，需要两个编码器/解码器对。

两对具有相同构造的编码器网络，各自在图像集上训练，编码器的参数在两个网络对之间共享。（有点encoder“求同”、decoder“存异”的味道）

![image-20210827101341252](E:\Document\Typora\img\image-20210827101341252.png)

该策略使通用编码器能够找到并学习两组面部图像之间的相似性，面部通常具有相似的特征（例如，眼睛，鼻子，嘴巴的位置），这是较为不具挑战性的。

两个网络使用相同的编码器编码，但使用不同的解码器训练解码过程（图3-2上部），人脸A的图像使用通用编码器编码，并使用解码器B解码，以创建一个deepfake（图3-2下部），人脸的角度、表情以及头型的相似度影响人脸替换的效果。

#### 3.1.2 卷积神经网络（CNN）

![image-20210827102119184](E:\Document\Typora\img\image-20210827102119184.png)

与全连接网络相反，CNN擅长学习数据中（局部）结构模式并组合得到高层次表征，因此在处理图像方面效率更高。

CNN中的卷积层学习的是卷积核／滤波器参数，这些滤波器在输入图像上移动，提取抽象的特征图作为输出。

随着网络变得越来越深，池化层降低维数，上采样层提高维数；它们可以灵活地构建用于图像的ED CNN。



#### 3.1.3 生成对抗网络（GAN）

![image-20210827103125114](E:\Document\Typora\img\image-20210827103125114.png)

GAN由Goodfellow等人于2014年首次提出，源于博弈论“零和博弈”思想，通过生成模型G (generative model) 和判别模型D (discriminative model) 互相博弈的方法来学习数据分布的生成式网络。

G旨在欺骗D来创建伪样本，它通过给定某种隐含信息，随机生成观测数据样本；D学会区分真实样本（x∈X）和伪样本（ xg = G(z) 其中z ∼ N），它预测数据样本是否属于真实训练样本。

具体来说，有一个分别用于训练D和G的对抗损失：

Ladv (D) = maxlogD(x) + log(1−D(G(z)))

Ladv (G) = minlog(1−D(G(z)))

在对抗博弈下，两者通过对抗式训练提升其能力，G学习如何生成与原始分布无法区分的样本。训练后，将D丢弃，并使用G生成内容。

最理想的状态是生成模型能够生成足以“以假乱真”的数据样本, 而判别模型却对其真伪性难以判别，即判断正确的概率只有 50%。

GAN优势是不依赖先验知识，生成模型的参数更新来自判别模型的反向传播，而非直接来自于数据样本，故训练不需要复杂的马尔科夫链。当应用于图像时，通常可以生成更高质量、逼真的图像样本。



#### 3.1.4 图像转换网络（Pix2Pix、CycleGAN）

Pix2Pix和CycleGAN是两种流行的使用GAN基本原理的图像转换网络。

（1）Pix2Pix可以完成从一个图像域到另一个图像域的转换。

![image-20210827102343202](E:\Document\Typora\img\image-20210827102343202.png)

在Pix2Pix中，G以输入图像x c作为输入，在给定目标标签x，希望G学习xc -> x的映射，即希望G所生成的图像xg和x无限接近，而D区分 (x, xc) 和 (xg, xc)。

Pix2Pix是一种监督式、成对式的训练方式，对数据有严苛要求，提升版本的Pix2PixHD可用来生成具有更好保真度的高分辨率图像。

（2）CycleGAN可以通过不成对的训练样本来进行图像转换。

![image-20210827102602620](E:\Document\Typora\img\image-20210827102602620.png)

该网络由两组GAN构成，并形成一个循环约束：将图像从一个域转换为另一个域，然后再次返回时，确保一致性。

#### 3.1.5、递归神经网络（RNN）

![image-20210827102732243](E:\Document\Typora\img\image-20210827102732243.png)

RNN可以处理序列和可变长度数据的神经网络，RNN的更高级版本包括长期短期记忆（LSTM）和门递归单元（GRU）。

在Deepfake制作中，RNN通常用于处理音频、视频。

### 3.2 特征表示

大多数神经网络结构使用一些中间表示来捕获、控制源身份s和目标身份t的面部结构、姿势和表情等，常见的各种方法如下。

l 使用面部动作编码系统（ facial action coding system，FACS）并测量面部的每个分类动作单元（AU）。

l 使用单目重建，从2D图像获得头部的3D可变形模型（3DMM），其中，姿势和表情使用矢量和矩阵参数化、头部使用参数或3D渲染。

l 使用头部或身体的UV图来使网络更好地了解形状的方向。

l 使用图像分割来帮助网络分离不同的语义区域（面部、头发等）。

l 最常见的表示形式是特征点（也称为关键点），即面部或身体上一组定义的位置点（利用开源CV库可有效跟踪）。

l 有的按通道分隔特征点，以使网络更容易标识和关联它们。

l 类似地，面部边界和身体骨骼点的表示等。

l 对于音频，常见方法是分割音频段，衡量音频段的Mel-Cepstral Coefficients（MCC）捕获主要语音频率。

### 3.3 泛化性

一个deepfake网络一般被设计或训练只针对特定的目标身份和源身份生效。身份不可知模型有时很难实现， 这是因为模型在训练期间学习了s和t的相关性。

定义E：表征x的特征或从x提取特征的模型或过程；M：一个训练后的模型，用于实现重现或替换；据此定义模型的3种泛化类别。

l one-to-one

用特定身份驱动特定身份：xg = Mt (Es (xs))。

l many-to-one

用任意身份驱动特定身份：xg = Mt (E (xs))。

l many-to-many

用任意身份驱动任意身份：xg = M (E1(xs), E2 (xt))。

### 3.4 技术挑战

l 泛化.性

生成网络是数据驱动的，因此在所生成的结果中反映了训练数据的特性；良好的效果受限于训练数据集，特定身份的高质量图像需要该身份的大量样本；获取驱动对象的训练数据通常比受害者的容易的多；如何最小化训练数据，并使训练过的模型在新的目标身份和源身份（非训练数据）上也生效。

l 成对式监督训练

带标签成对具有GT（ground truth，正确打标记的数据）的数据实在难以获取。对此，许多deepfake网络通过使用自监督的方式进行训练，或者使用不成对的网络（例如CycleGAN），或者利用ED网络的编码在潜在空间进行特征编辑。

l 非期望特征迁移

有时会把源身份的特征或区域迁移到目标身份上，对此提出使用注意力机制、few-shot学习，解耦学习、跳跃连接等方法将更相关的信息传递给生成器。

l 遮挡

遮挡可能是手、头发、眼镜或任何其它物品，还有眼睛或嘴巴被隐藏或动态变化，这可能导致（裁剪的图像、不一致的面部特征）出现伪影、不合理的生成效果等，对此提出对障碍物进行分割和修补。

l 时间连贯性

Deepfake视频通常会产生更明显的伪影，例如闪烁和抖动，是因为大多数deepfake网络无先前帧的上下文、单独处理每个帧，对此提出将上下文提供给G和D，或进行时间连贯损失约束，或使用RNN，或对它们进行组合使用。

### 3.5、深度伪造制作

#### 3.5.1、制作方法折衷

每个制作方法有不同的成本和收益，最有效和最具威胁性的方法是：（1）最具实施性（训练数据、执行速度、可访问性）；（2）对受害者最可信（质量／可信度）。

l 数据&质量

用大量数据训练的模型产出的效果通常更好，但需许多小时的训练素材，仅适用于知名人士。

对任意个人的攻击需使用具有many-to-many泛化能力的模型或少样本学习方法，这类模型必须“想象”缺少的信息（例如姿势、遮挡），通过提供数量有限的参考样本可达成数据和质量之间的折衷。

l 速度&质量

这两者之间的取舍取决于攻击是在线（交互式）或离线（存储媒体），此类社会工程学攻击通常是在线的，需要实时速度。

但高分辨率的模型具有许多参数、可能会使用多个网络、可能会处理多个帧以提供时间连贯性；其它方法可能会因预处理／后处理步骤变慢。

已知的声称能实时产生伪造品的模型在主观上趋于模糊或扭曲面部。当在遭受社会工程学攻击的虚假误导压力之下，不完美的伪造品很可能对受害者是有效的。

攻击者很可能在低分辨率水平上实施复杂的方法以加快帧速率。对于非实时攻击，分辨率和保真度重要，具有时间连贯性的高质量图像和视频是最佳选择。

l 可访问性&质量

可访问性和可复现性是新技术扩散的关键因素，在网络公开发布的代码和数据更有可能被研究人员和罪犯使用，这是因为相比使用网络上有效可用的方法，复现论文的收益很小。

考虑以上因素，以下是目前最重要和最可用的深度伪造技术：

（1）面部重现：高效和实用性；

（2）嘴巴重现：质量高；

（3）人脸替换：高保真度和广泛传播使用。

#### 3.5.2 当前的局限性

除了质量，当前的深度伪造制作还存在一些局限性技术。

l 重现总是以正面姿势驱动和生成

这导致重现的表现非常静态，可通过换脸把身份交换到相似的身体上避免，但不一定总能做到很好的匹配，实用性有限。

l 重现和替换依赖驱动器的表现来交付身份的特性

下一代的深度伪造将利用目标的视频生成具有期望的表情和举止的内容，使得创建令人信服的深度伪造过程更加自动化。

l 连贯渲染

头发、牙齿、舌头、阴影的连贯渲染，以及渲染目标目标手的能力（尤其是在触摸脸部时）。

l 原生分辨率

![image-20210827104606663](E:\Document\Typora\img\image-20210827104606663.png)

用于训练的数据原生分辨率每增长P，则训练时长至少增加P^2。

#### 3.5.3 基本制作流程

总体分为三个核心步骤：数据收集、模型训练、伪造内容生成。

l 数据收集

大量源和目标人物的人脸照片，尽可能覆盖人脸的各种角度。

l 模型训练

训练耗时依赖硬件配置、训练数据的质量。

l 伪造内容生成

对于生成目标xg，重现和人脸替换类型的网络一般遵循以下流程（将x传递到以下pipeline）：

![image-20210827104734650](E:\Document\Typora\img\image-20210827104734650.png)

（1）检测并裁剪面部；

（2）提取中间特征表示；

（3）根据一些驱动信息（如另一张脸）生成新的面部内容；

（4）将生成的脸融合到目标帧中。

l 通常使用的6种驱动图像生成的方法

（1）让网络直接在图像上执行映射学习；

（2）使用ED解耦身份和表情并在传递给解码器前修改／交换编码特征；

（3）在将其传递给解码器之前添加其它编码（如AU、embedding）；

（4）在生成之前将中间人脸／身体的特征表示转换为所需的身份／表情；

（5）通过源视频帧序列的光流场驱动生成；

（6）使用3D渲染、扭曲的图像或生成的内容进行组合，以创建原始内容（头发、场景等）的合成，然后将该合成（粗略结果）传递给另一个网络（ 例如pix2pix）以改善真实感。

#### 3.5.4、换脸工具－FakeApp

Reddit社区原始的deepfake使用ED网络：1个编码器En，2个解码器Des和Det，作为2个autoencoders同时训练：Des (En(xs)) = Xs，Det (En(xt)) = Xt，其中x是裁剪出的人脸图像。En学会将s和t映射到共享的潜在空间：Des(En(xt)) = xg。

基于Reddit原始网络的更强大开源换脸工具有FaceSwap（34K星）、DeepFaceLab（23K星，95+% deepfakes的制作工具）、FaceSwap-GAN（3K星）等，支持了更多的模型，这些模型泛化能力均是one-to-one。

![image-20210827104829910](E:\Document\Typora\img\image-20210827104829910.png)

##### 3.5.4.1 基本流程

![image-20210827104900554](E:\Document\Typora\img\image-20210827104900554.png)

（1）数据收集

收集大量目标人脸的原始照片，提取目标人脸；

（2）模型训练

对于64px input，64px output，训练原始模型GPU耗时约12-48小时，比如（CUDA + GPU + TensorFlow），CPU耗时约数周；

![image-20210827104946246](E:\Document\Typora\img\image-20210827104946246.png)

​                                                          基本的编码－解码网络训练过程

![image-20210827105026112](E:\Document\Typora\img\image-20210827105026112.png)

​                                                           自动编码解码网络训练过程

用2个数据集训练共享的Encoder和 2个Decoder，Decoder A试图重建Face A、Decoder B试图重建Face B；

（3）伪造内容生成：向训练过的模型输入人脸图像，用模型输出的人脸，混合生成伪造内容。

![image-20210827105100249](E:\Document\Typora\img\image-20210827105100249.png)

​                                                                    人脸转换过程

# Deepfacelab使用

[【停更】Deepfacelab 新手教程_马晓柴是个可爱的男孩子-CSDN博客](https://blog.csdn.net/u014575897/article/details/85272910)

[AI黑科技：Deepfacelab(人工智能换脸合成技术)的简介、安装、使用方法之详细攻略【图文】_一个处女座的程序猿_51CTO博客](https://blog.51cto.com/u_14217737/2913032)



其实整个逻辑是先把视频解码成一帧帧的数据

## Q:运行到从图片中提取人像

这个教程比较好用

[deepfacelab使用教程【附详细安装教程+安装包】_洪荒少女66的博客-CSDN博客_deepfacelab](https://blog.csdn.net/weixin_56090697/article/details/115792226)



`DeepFaceLabOpenCLSSE`是CPU用来跑AI换脸的

`DeepFaceLabCUDA10.1AVX`是GPU用来跑AI换脸的

冷知识：

GeForce RTX 30系显卡目前是支持CUDA 11.1及以上版本，而Tensorflow 2.4及更高版本才支持CUDA 11~

所以本机不能跑QAQ



参考这个

[AI换脸教程：DeepFaceLab使用教程（1.安装及分解视频） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/98173273)

[AI换脸教程：DeepFaceLab使用教程（2.训练及合成） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/98174121)

## Q:报错 H64模型训练时报错

failed to create cublas handlethis is probably because cuDNN failed to initialize

依靠我的机智勇敢聪明，按照这个方法

[问题处理failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED_smallwhite620451的博客-CSDN博客](https://blog.csdn.net/smallwhite620451/article/details/114666340)

更改`minscripts/Trainer.py`的main函数

~~~
# 头部分
import tensorflow as tf

# main的函数
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.InteractiveSession(config=config)
~~~

关于ConfigProto

[tensorflow学习笔记（二十五）：ConfigProto&GPU_Keith-CSDN博客](https://blog.csdn.net/u012436149/article/details/53837651)

实验室数据 (file://Desktop-00mr3dj/实验室数据)



## Q:merge无效

下载了专用于30系列的T卡的deepfacelab

![image-20211015151424118](E:\Document\Typora\img\image-20211015151424118.png)

merge卡在了这一步，发现报错

~~~
Exception ignored in: <bound method Buckets.__del__ of <tensorflow.python.eager.monitoring.ExponentialBuckets object at 0x000002BE34B20FA8>>
Traceback (most recent call last):
  File "E:\Download\DeepFaceLab_RTX_3000_build\DeepFaceLab_NVIDIA\_internal\python-3.6.8\lib\site-packages\tensorflow\python\eager\monitoring.py", line 407, in __del__
AttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'
~~~



好像是tensorflow的一个错误造成的

[python - 无法将 tf 模型转换为 tflite 模型 - 堆栈内存溢出 (stackoom.com)](https://stackoom.com/question/4N9tf)









# Deepfacelab代码分析

## 基础知识学习

[Python中import的用法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/63143493)

[Python 为什么要继承 object 类？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/19754936)

[python的class(类)中继承object 与不继承的区别_程序猿踩坑集锦-CSDN博客](https://blog.csdn.net/qq_27828675/article/details/79358893)

[Python类中属性、方法与self的关系_爱吃干脆面的小潘-CSDN博客](https://blog.csdn.net/p1306252/article/details/107447261?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.no_search_link&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.no_search_link)

### 嵌套函数

[python函数def里面嵌套def_python 函数嵌套函数_Python中的嵌套函数_神贱腹黑兔的博客-CSDN博客](https://blog.csdn.net/weixin_36483301/article/details/114417543?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link)

[python中函数嵌套、函数作为变量、以及闭包的原理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/163008321)

### python

#### np.newaxis

np.newaxis的作用就是在这一位置增加一个一维，这一位置指的是np.newaxis所在的位置。

```bash
In [124]: arr = np.arange(5*5).reshape(5,5)

In [125]: arr.shape
Out[125]: (5, 5)

# promoting 2D array to a 5D array
In [126]: arr_5D = arr[np.newaxis, ..., np.newaxis, np.newaxis]

In [127]: arr_5D.shape
Out[127]: (1, 5, 5, 1, 1)
```

#### np.repeat()

**np.repeat()用于将numpy数组重复。**

**numpy.repeat(a, repeats, axis=None);**

参数：

axis=0,沿着y轴复制，实际上增加了行数
axis=1,沿着x轴复制，实际上增加了列数

~~~
c=np.array(([1,2],[3,4]))
c=np.repeat(c,3,-1)
print(c)
[[1 1 1 2 2 2]
 [3 3 3 4 4 4]]
~~~

#### pass

Python pass 是空语句，是为了保持程序结构的完整性。

**pass** 不做任何事情，一般用做占位语句，可以继承之后置空。

类似于纯虚函数？

~~~
class AAA:
    def __init__(self):
        self.a = 1

    def color(self):
        self.b = self.a
        print(self.b)

class BBB(AAA):
    def __init__(self):
        self.a = 2

    def color(self):
        pass

if __name__ == "__main__":
    b = BBB()
    b.color()
    a = AAA()
    a.color()
~~~

~~~
运行结果：
1
~~~





#### spawn

~~~
multiprocessing.set_start_method("spawn")
~~~

[pytorch中使用cuda进行多任务multiprocessing_YNNAD1997的博客-CSDN博客](https://blog.csdn.net/YNNAD1997/article/details/113829532)





## 项目结构

~~~
Deepfacelab
├── _internal 		# 存放代码和依赖库
├───── CUDA
├───── CUDNN
├───── DeepFaceLab	# deepfacelab源代码
├──────── main.py    # 脚本调用文件
├──────── mainscripts	# main.py调用函数的路径
├───── python
├── workspace		# 运行路径
├── 运行脚本
~~~

## 脚本代码

`call **.bat`调用某个脚本后返回

`2>nul`屏蔽错误信息

### setenv.bat

~~~
SET INTERNAL=%~dp0
~~~

- %0代表文件本身
- d代表盘符
- p代表路径

~~~
SET INTERNAL=%INTERNAL:~0,-1%
~~~

%PATH:~10,5%

会扩展 PATH 环境变量，然后只使用在扩展结果中从第 11 个(偏
移量 10)字符开始的五个字符。如果没有指定长度，则采用默认
值，即变量数值的余数。

%PATH:~-10%

会提取 PATH 变量的最后十个字符。

%PATH:~0,-2%

会提取 PATH 变量的所有字符，除了最后两个。

~~~
pause
~~~

dos命令pause教程，暂停bat批处理脚本程序，请按任意键继续

## 依赖库配置

版本配置

~~~
Tensorflow 2.4.0 + CUDA 11.2 + CuDNN 8.0.5
~~~

##  main.py

[argparse --- 命令行选项、参数和子命令解析器 — Python 3.10.0 文档](https://docs.python.org/zh-cn/3/library/argparse.html)

`parser = argparse.ArgumentParser()`: 创建一个parser对象。

`subparsers = parser.add_subparsers()`: 给parser对象一个子对象。

`add_argument`添加参数，定义单个的命令行参数应当如何解析。

- [name or flags](https://docs.python.org/zh-cn/3/library/argparse.html#name-or-flags) - 一个命名或者一个选项字符串的列表，例如 `foo` 或 `-f, --foo`。
- [action](https://docs.python.org/zh-cn/3/library/argparse.html#action) - 当参数在命令行中出现时使用的动作基本类型。
- [nargs](https://docs.python.org/zh-cn/3/library/argparse.html#nargs) - 命令行参数应当消耗的数目。
- [const](https://docs.python.org/zh-cn/3/library/argparse.html#const) - 被一些 [action](https://docs.python.org/zh-cn/3/library/argparse.html#action) 和 [nargs](https://docs.python.org/zh-cn/3/library/argparse.html#nargs) 选择所需求的常数。
- [default](https://docs.python.org/zh-cn/3/library/argparse.html#default) - 当参数未在命令行中出现并且也不存在于命名空间对象时所产生的值。
- [type](https://docs.python.org/zh-cn/3/library/argparse.html#type) - 命令行参数应当被转换成的类型。
- [choices](https://docs.python.org/zh-cn/3/library/argparse.html#choices) - 可用的参数的容器。
- [required](https://docs.python.org/zh-cn/3/library/argparse.html#required) - 此命令行选项是否可省略 （仅选项可用）。
- [help](https://docs.python.org/zh-cn/3/library/argparse.html#help) - 一个此选项作用的简单描述。
- [metavar](https://docs.python.org/zh-cn/3/library/argparse.html#metavar) - 在使用方法消息中使用的参数值示例。
- [dest](https://docs.python.org/zh-cn/3/library/argparse.html#dest) - 被添加到 [`parse_args()`](https://docs.python.org/zh-cn/3/library/argparse.html#argparse.ArgumentParser.parse_args) 所返回对象上的属性名。

添加程序参数信息是通过调用 [`add_argument()`](https://docs.python.org/zh-cn/3/library/argparse.html#argparse.ArgumentParser.add_argument) 方法完成的。通常，这些调用指定 [`ArgumentParser`](https://docs.python.org/zh-cn/3/library/argparse.html#argparse.ArgumentParser) 如何获取命令行字符串并将其转换为对象。

~~~
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers()
    
    videoed_parser = subparsers.add_parser( "videoed", help="Video processing.").add_subparsers()	# videoed_parser是解析videoed其后命令的子模块

    def process_videoed_extract_video(arguments):
        osex.set_process_lowest_prio()
        from mainscripts import VideoEd
        VideoEd.extract_video (arguments.input_file, arguments.output_dir, arguments.output_ext, arguments.fps)
    p = videoed_parser.add_parser( "extract-video", help="Extract images from video file.")
    p.add_argument('--input-file', required=True, action=fixPathAction, dest="input_file", help="Input file to be processed. Specify .*-extension to find first file.")
    p.add_argument('--output-dir', required=True, action=fixPathAction, dest="output_dir", help="Output directory. This is where the extracted images will be stored.")
    p.add_argument('--output-ext', dest="output_ext", default=None, help="Image format (extension) of output files.")
    p.add_argument('--fps', type=int, dest="fps", default=None, help="How many frames of every second of the video will be extracted. 0 - full fps.")
    p.set_defaults(func=process_videoed_extract_video)
~~~

一个特别有效的处理子命令的方式是将 [`add_subparsers()`](https://docs.python.org/zh-cn/3/library/argparse.html#argparse.ArgumentParser.add_subparsers) 方法与对 [`set_defaults()`](https://docs.python.org/zh-cn/3/library/argparse.html#argparse.ArgumentParser.set_defaults) 的调用结合起来使用，这样每个子解析器就能知道应当执行哪个 Python 函数。

以上命令的意思是`videoed`命令信息触发解析子模块，然后触发`process_videoed_extract_video`函数，调用

~~~
from mainscripts import Sorter
~~~

`mainscripts`为路径的名字，所以调用该目录下的文件



### interact.py--交互界面

`main.py`调用

~~~
    from core.interact import interact as io
~~~

通过本地配置，导入io交互。

~~~
try:
    import IPython #if success we are in colab
    from IPython.display import display, clear_output
    import PIL
    import matplotlib.pyplot as plt
    is_colab = True
except:
    is_colab = False
~~~

根据是否能导入外部变量，初始化变量`is_colab`，有点类似于

~~~
if is_colab:
    interact = InteractColab()
else:
    interact = InteractDesktop()
~~~

`InteractDesktop()`应该是我看到的这个图

![image-20211108110009644](E:\Document\Typora\img\image-20211108110009644.png)





## 2) extract images from video data_src

`videoed extract-video`调用`process_videoed_extract_video`函数，即调用`mainscripts`路径下的`VideoEd`的`extract_video`函数。



### python调用ffmpeg

~~~
    job = ffmpeg.input(str(input_file_path))

    kwargs = {'pix_fmt': 'rgb24'}
    if fps != 0:
        kwargs.update ({'r':str(fps)})

    if output_ext == 'jpg':
        kwargs.update ({'q:v':'2'}) #highest quality for jpg

    job = job.output( str (output_path / ('%5d.'+output_ext)), **kwargs )

    try:
        job = job.run()
    except:
        io.log_err ("ffmpeg fail, job commandline:" + str(job.compile()) )
~~~



错误打印

![image-20211104160739252](E:\Document\Typora\img\image-20211104160739252.png)

应该是启动python可执行文件时报错的

~~~
AttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'
~~~



## 4) data_src faceset extract MANUAL

`4) data_src faceset extract MANUAL`默认的detect模式为`manual`，手动标记面部图像。

`4) data_src faceset extract`默认的detector模式为`s3fd`，自动去生成剪裁的脸。

调用`mainscripts/Extractor.py`的`main`函数

`ExtractSubprocessor`调用`Cli`



## 5) data_dst faceset extract

~~~
5) data_dst faceset extract + manual fix     	--->detector s3fd manual fix

    --output-debug ^
    --detector s3fd ^
    --max-faces-from-image 0 ^
    --manual-fix

5) data_dst faceset extract MANUAL				--->detector manual

    --detector manual ^
    --max-faces-from-image 0 ^
    --output-debug

5) data_dst faceset extract						--->detector s3fd

    --detector s3fd ^
    --max-faces-from-image 0 ^
    --output-debug

5) data_dst faceset MANUAL RE-EXTRACT DELETED ALIGNED_DEBUG		--->detector manual

    --detector manual ^
    --max-faces-from-image 0 ^
    --output-debug ^
    --manual-output-debug-fix
~~~

`s3fd`是自动采集面部数据，`manual`是主动选择面部。

~~~
    if images_found != 0:
        if detector == 'manual':
            io.log_info ('Performing manual extract...')
            data = ExtractSubprocessor ([ ExtractSubprocessor.Data(Path(filename)) for filename in input_image_paths ], 'landmarks-manual', image_size, jpeg_quality, face_type, output_debug_path if output_debug else None, manual_window_size=manual_window_size, device_config=device_config).run()

            io.log_info ('Performing 3rd pass...')
            data = ExtractSubprocessor (data, 'final', image_size, jpeg_quality, face_type, output_debug_path if output_debug else None, final_output_path=output_path, device_config=device_config).run()

        else:
            io.log_info ('Extracting faces...')
            data = ExtractSubprocessor ([ ExtractSubprocessor.Data(Path(filename)) for filename in input_image_paths ],
                                         'all',
                                         image_size,
                                         jpeg_quality,
                                         face_type,
                                         output_debug_path if output_debug else None,
                                         max_faces_from_image=max_faces_from_image,
                                         final_output_path=output_path,
                                         device_config=device_config).run()
~~~

`ExtractSubprocessor`是`Subprocessor`的子类，调用`Subprocessor`的`run`函数

![image-20211105155842916](E:\Document\Typora\img\image-20211105155842916.png)

![image-20211105155936922](E:\Document\Typora\img\image-20211105155936922.png)

主要的面部特征选择的代码为

~~~
data = ExtractSubprocessor ([ ExtractSubprocessor.Data(Path(filename)) for filename in input_image_paths ], 'landmarks-manual', image_size, jpeg_quality, face_type, output_debug_path if output_debug else None, manual_window_size=manual_window_size, device_config=device_config).run()
~~~

~~~
class Subprocessor
├── class Cli
├──── 子线程：_subprocess_run
~~~

`_subprocess_run`子线程用于处理函数







### 不理解

~~~
        for name, host_dict, client_dict in self.process_info_generator():
            try:
                cli = self.SubprocessorCli_class(client_dict)
                cli.state = 1
                cli.sent_time = 0
                cli.sent_data = None
                cli.name = name
                cli.host_dict = host_dict

                self.clis.append (cli)

                if self.initialize_subprocesses_in_serial:
                    while True:
                        cli_init_dispatcher(cli)
                        if cli.state == 0:
                            break
                        io.process_messages(0.005)
            except:
                raise Exception (f"Unable to start subprocess {name}. Error: {traceback.format_exc()}")
~~~



## 6) train SAEHD

跟`extract`一样的代码逻辑，`c2s`和`s2c交互`

__import__是python的一个内置方法，直接调用__import__()即可获取一个模块

~~~
 import(name, globals=None, locals=None, fromlist=(), level=0)
~~~

[python __import__内置函数使用_cswhl的博客-CSDN博客](https://blog.csdn.net/cswhl/article/details/111710876)



#### 报错

~~~
TypeError: Can't parse 'center'. Sequence item with index 0 has a wrong type
~~~

位置

内容`random_transform_mat = cv2.getRotationMatrix2D((w // 2, w // 2), rotation, scale)`





# SimSwap

[简单介绍SimSwap（类似DeepFaceLab）单张图视频换脸的项目_Shion's bla bla blog-CSDN博客](https://blog.csdn.net/ddrfan/article/details/119144500?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-1.no_search_link&spm=1001.2101.3001.4242)

配置`simswap`环境





# 数据集收集

![image-20211119155559693](E:\Document\Typora\img\image-20211119155559693.png)





## 修改的逻辑

通常的篡改分为四类:

1）全脸篡改（都是GAN生成的，能生成形形色色的不存在的人及表情姿势）

2）身份更改（也就是通常所说的deepfake，仅仅更改人脸，faceswap也属于该范畴）

3）属性更改（人脸身份不变，但是属性发生改变，包括发色、添加眼镜的配件）

4）表情更改（要是face2face，用target用户表情篡改source用户，而且可以达到实时的效果）

[deepfake篡改取证综述-(1)数据集介绍 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/266187716)



## UADFV

在所有的假视频中只考虑一个身份。每个视频代表一个人，典型分辨率为294×500像素，平均11.14秒。



## Deepfake-TIMIT数据库

来自VidTIMIT数据库的620个假视频，共32个主题。假视频是利用公共GAN的人脸交换al-gorithm制作的。在该方法中，生成网络采用了CycleGAN[71]，使用FaceNet[65]的权重。多任务级联卷积网络方法用于更稳定的检测和可靠的面部对齐[85]。此外，还考虑了Kalman滤波来平滑边界框在帧上的位置，并消除了交换面上的抖动。对于DeepfakeTIMIT中考虑的场景，考虑两种不同的质量：i）低质量（LQ），图像为64×64像素；ii）高质量（HQ），图像为128×128像素。此外，不同的混合技术被应用到假视频的质量水平。



## FaceForensics++

FaceForensics++包含1000个从Youtube中提取的真实视频。关于身份交换假视频，它们是使用计算机图形和DeepFake方法（即学习方法）生成的。对于计算机图形方法，作者考虑了公开可用的FaceSwap算法，而对于DeepFake方法，假视频是通过DeepFake FaceSwap GitHub创建的。人脸交换方法包括人脸对齐、高斯-牛顿优化和图像融合，将源人物的脸交换到目标人。



## DeepFakeDetection

 该数据集包含在了FaceForensics++中，不过质量更高）



## Celeb DF

(使用高频数据库，种类比较多)

这个数据库旨在提供视觉质量更好的假视频，类似于在互联网上共享的流行视频，与以前的数据库相比，这些数据库显示了低视觉质量和许多可见的人工制品。Celeb DF包括从Youtube提取的890个真实视频和5639个假视频，这些视频是通过公共DeepFake生成算法的改进版本创建的，改善了合成人脸的低分辨率和颜色不一致等问题。



# ForgeryNet

+ Advantage
  + wild original data
    + angle, expression, identity, lighting, scenario  datasets[6,10,14,32]
  + Various Forgery Approaches  
  + Diverse Re-rendering Process 
  + Rich Annotations and Comprehensive Tasks  

+ first generation datasets
  + DF-FIMIT[25], UADFV[43], SwapMe and FaceSwap[47]
    + DF-TIMIT  with faces swapped
    + UADFV generated by FakeApp
    + SwapMe and FaceSwap choose two face swapping Apps [1,2] to create 2010 forgery images in total on 1005 original real images. 

![image-20211122105322245](E:\Document\Typora\img\image-20211122105322245.png)

+ second generation datasets
  + **Google DeepFake Detection** dataset [4]  with 3,068 forgery videos by five publicly available manipulation approaches 
  + ✔️**Celeb-DF** [27] containing 590 YouTube real videos mostly from celebrities
    and 5,639 manipulated video clips.   
  + **FaceForensics++** [38]consists of 4000 fake videos manipulated by four approaches (i.e. DeepFakes, Face2Face, FaceSwap and NeuralTextures), and 1000 real videos from YouTube.  
    + [TUM Visual Computing & Artificial Intelligence: Prof. Matthias Nießner (niessnerlab.org)](http://www.niessnerlab.org/projects/roessler2019faceforensicspp.html)
    + 

+ third generation datasets

  + ✔️**DeeperForensics-1.0** [24] consists of 60,000 videos for real-world face forgery detection.  

  + **DFDC**[12] contains over 100,000 clips sourced from 960 paid actors, produced with several face replacement forgery approaches including learnable and non-learnable approaches.  

    + [Deepfake Detection Challenge Dataset (facebook.com)](https://ai.facebook.com/datasets/dfdc/)
    + aws注册：XYGxsj666!

  + **DFFD**[11] provides annotations of spatial forgery at the first time, yet it only presents binary
    masks without manipulation density.  

    + Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, and Anil K Jain. On the detection of digital face manipulation.In CVPR, pages 5781–5790, 2020.  
    + [Diverse Fake Face Dataset (msu.edu)](http://cvlab.cse.msu.edu/dffd-dataset.html)

    ~~~
    DFFD Dataset Application
    Thank You. You can now access the dataset using username: "dffd_dataset" and password: "cvlab-dataset-dffd-0614"
    ~~~

    + [NVlabs/ffhq-dataset: Flickr-Faces-HQ Dataset (FFHQ) (github.com)](https://github.com/NVlabs/ffhq-dataset)
    + FFHQ为GAN生成的虚假图像
    + ![image-20211122150957609](E:\Document\Typora\img\image-20211122150957609.png)

+ LINK [A literature review of deepfakes(三) 深度学习换脸数据集&综述文章 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/92853899)

+ 

+ Original four dataset data

  + face identity, angle, expression, scenarios

  + CREMAD [6], RAVDESS [32], VoxCeleb2 [10] and AVSpeech [14]

    + CREMAD ：Houwei Cao, David G Cooper, Michael K Keutmann, Ruben C Gur, Ani Nenkova, and Ragini Verma. Crema-d:Crowd-sourced emotional multimodal actors dataset.  
    + RAVDESS ：The ryerson audio-visual database of emotional speech and song
      (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english.
    + VoxCeleb2 ：Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. Voxceleb2: Deep speaker recognition
    + AVSpeech ：Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation.

    
